---
title: "Custom SEM"
format: pdf
editor: visual
---

# Setup

```{r}
#| message: false
#| warning: false

# install.packages('TMB', type = 'source', lib = .libPaths()[2]) 
library('RTMB', lib.loc = .libPaths()[1])
library('tidyverse')
library(xtable)
library(CatDyn)
library(tmbstan)
library(shinystan)

load("data/df_effort_m_mbw_otb.Rdata")
# sample data for testing purposes
iotc = read.csv('data/IOTC-DATASETS-2025-03-13-CELongline.csv')

iotc =
iotc %>% filter(Fleet == 'JPN    ') %>%
  filter(!is.na(YFT.NO)) %>% 
  group_by(Year, MonthStart) %>% 
  summarise(Effort = sum(Effort, na.rm =T),
            Catch = sum(YFT.NO, na.rm = T ) *0.01955034)

precip = c(-16.67,
           115.33, 245.93, 193.93, -229.47,-76.47,
           250.33, 177.73, 115.63, 90.13, -299.57,
           -338.37,97.18, -317.67, -198.74,-14.07,
           221.67, -90.97, -205.97, 98.03, 256.73,
           -241.87, 150.13, -300.17, 98.43, -85.87,
           -94.67,-156.87,-43.87,-106.37) + 
  735.8 + 105.7
anos = 1994:2023

mod_aux = lm(df_effort$catch ~ df_effort$effort)

df_effort = 
  df_effort %>% 
  mutate(catch = case_when(year_sale == 2005 &
                             month_sale == '09' ~
                             effort * mod_aux$coefficients[2] +
                             mod_aux$coefficients[1],
                           T ~ catch),
         catch_otb = case_when(year_sale == 2005 &
                                 month_sale == '09' ~
                                 effort_otb * mod_aux$coefficients[2] +
                                 mod_aux$coefficients[1],
                               T ~ catch_otb)) %>% 
  mutate(catch_otb = case_when(catch_otb == 0 ~ mean(catch_otb), 
                               T ~ catch_otb),
         effort_otb = case_when(effort_otb  < 150 ~ mean(effort_otb),
                                T ~ effort_otb))

```

### Plotting Biomass estimates

```{r}
#| eval: false
ggplot() +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$catch_predictions),
            color = 'red') +
  geom_point(aes(x = 1:length(dat$Ct),
                y = dat$Ct)) +
  theme_bw()



temp = data.frame(x = obj4$report()$biomass_predictions) %>% 
  mutate(y = sdr4$par.fixed[['r']]*x *
           (1-(x/sdr4$par.fixed[['K']])))

ggplot() +
  geom_point(aes(y  = temp$y,
                x = temp$x),
            color = 'red') +

  theme_bw()


ggplot() +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$biomass_predictions),
            color = 'purple') +  
  geom_point(aes(x = 1:length(dat$Ct),
                y = dat$Ct)) +
  theme_bw()
```

# Foreword

# Data description and context

## Catch $C_t$

Catch is obtained via reported landings. It can converted to number of individuals via a mean body weight model (not described here), which is prefered in models such as generalized depletion models (GDMs).

For the purposes of this course, 2 outliers were forcefully removed from the dataset. One pertained to an imposed closure on the fishery, while the other was an abnormally low activity month, with 4 fishing days in the entire fleet and 0 octopus landed. In a serious report on this data these gaps would have to be explained and accounted for, but for simplicity sake here they were replaced by the linear prediction of the series in the first outlier (catch-only) and the mean of the series (abnormal catch and effort).

```{r}
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y = catch_otb/1000)) + 
  theme_bw() + 
  labs(title = 'nominal catches (tons)')
```

## Effort $E_t$

Since we are dealing with a polyvalent fishery that employs a wide range of fishing gears and operates in vessels that are small enough to be exempt from keeping logbooks, effort data can be estimated at best by attempting to count fishing days in each month. Outliers in this series were dealt with as described earlier.

```{r}
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y =effort_otb)) + 
  theme_bw() + 
  labs(title = 'Effort (fishing days)')
```

## Catch - Effort Dynamics

```{r}
df_effort %>% 
  ggplot() + 
  geom_point(aes(x = effort_otb,
                y = catch_otb)) + 
  theme_bw() + 
  labs(title = 'Catch ~ Effort')
```

```{r}

modelo = lm(df_effort$catch_otb/df_effort$effort_otb ~ df_effort$effort_otb)

a = modelo$coefficients[["df_effort$effort_otb"]]
b = modelo$coefficients[['(Intercept)']]

df_effort %>% 
ggplot() + 
  geom_point(aes(x = effort_otb,
                 y = catch_otb/effort_otb)) + 
  geom_abline(slope = a, intercept = b, color = 'red') + 
  theme_bw()

novo = df_effort %>% 
  mutate(
  cpue_hat = predict(modelo, newdata = df_effort),
                  Y = cpue_hat * df_effort$effort_otb)

novo %>% 
  ggplot() +
  geom_point(aes(x = effort_otb,
                 y = catch_otb), col = 'red') + 
  geom_point(aes(x = effort_otb,
                 y = Y)) + theme_bw()

```

## Precipitation $P_t$

Just total rainfall in Algarve throught the series period. Unlike the other data, it has an yearly time step. This was done deliberatly, to accommodate the challenge of integrating data with mismatching time steps, something that occurs far too often.

```{r}
  ggplot() + 
  geom_line(aes(x = anos,
                y = precip)) + 
  theme_bw() + 
  labs(title = 'Yearly precipitation')
```

## Mean body weight

Mean body weight model used to convert catch in weight to numbers. Based on samples collected at auction markets.

```{r}
df_effort %>% 
  ggplot() +
  # geom_line(aes(x = week,
  #               y = mbw,
  #               group = year_sale)) +
  geom_line(aes(x = month_sale,
                y = mbw_rand,
                group = year_sale,
                color = year_sale)) + 
  # scale_color_manual(values = cores2) +
  # facet_wrap(year_sale ~.) + 
  theme_bw() + 
  theme(legend.position = 'none') +
  labs(y = 'mean body weight (kg)')
```

# First act: I flunked my time series course twice

Mention on how some correlation was observed

## Model 1: Baseline

In this model we estimate $phi$ and $C_{0}$ (which the catch at $t = 0$ and not actually part of the catch series, hence why the need for estimation).

$$
C_{t+1} = \phi \cdot C_t  + \epsilon_t
$$

```{r}
#| eval: true
#| results: hide
dat = list()

dat$y = as.vector(df_effort$catch_otb)  # catches

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll

  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0}
    else{predY = phi*y[i-1]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")
```

```{r}
#| output: asis
summary(sdr) %>% as.data.frame() %>% xtable %>% print(comment = F)
```

We can calculate the AIC and wAIC: these values are mighty suspicious

```{r}
# Calculate AIC
logLik_value = -fit$objective
k = length(fit$par)
aic_value = 2 * k - 2 * logLik_value

# Calculate WAIC
log_lik = numeric(length(dat$Y))

# Vector of predicted means
meanvec = c(pl$y0)
  for(i in 2:length(dat$y)){
    meanvec[i] = tanh(pl$atanh_phi)*meanvec[i-1]}
    #meanvec[i] = tanh(pl$atanh_phi) %*% t(meanvec[i-1])} 

# Revert transformation of sigma(y)  
sdy = exp(pl$logsdY)

for(i in 1:length(dat$y)){
  log_lik[i] = dnorm(dat$y[i], meanvec[i], sdy, log=TRUE)
}

lppd = sum(log(mean(exp(log_lik))))
p_waic = sum(var(log_lik))
waic = -2 * (lppd - p_waic)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = 'Model 1',
                  AIC = aic_value, 
                  WAIC = waic)) %>% print(comment = F)
```

```{r}
ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) + 
  theme_bw() +
  labs(title = 'data (points) and model predictions (line)' )
```

## Model 2: With Precipitation

In this version, we added rainfall $P_t$ from the previous year to the model. The series is actually lagged, so any given $P_{1995}$ will actually refer to the year of 1994, *eg*. This was done for ease of integration.

$$
C_{t+1} = \phi \cdot C_t + \alpha\cdot P_t + \epsilon_t
$$

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

par$alpha = 1 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj2 = MakeADFun(jnll, par)
fit2 = nlminb(obj2$par, obj2$fn, obj2$gr)

sdr2 = sdreport(obj2)
pl2 = as.list(sdr2,"Est")
plsd2 = as.list(sdr2,"Std")
```

```{r}
#| output: asis
summary(sdr2) %>% as.data.frame() %>% xtable %>% print(comment = F)
```

```{r}
# Calculate AIC
logLik_value2 = -fit2$objective
k = length(fit2$par)
aic_value2 = 2 * k - 2 * logLik_value2

# Calculate WAIC
log_lik2 = numeric(length(dat$Y))

# Vector of predicted means
meanvec2 = c(pl2$y0)
  for(i in 2:length(dat$y)){
    meanvec2[i] = tanh(pl2$atanh_phi)*meanvec[i-1] + pl2$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl2$logsdY)

for(i in 1:length(dat$y)){
  log_lik2[i] = dnorm(dat$y[i], meanvec2[i], sdy, log=TRUE)
}

lppd2 = sum(log(mean(exp(log_lik2))))
p_waic2 = sum(var(log_lik2))
waic2 = -2 * (lppd2 - p_waic2)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt'),
                  AIC = c(aic_value, aic_value2), 
                  WAIC = c(waic, waic2))) %>% print(comment = F)
```

Black is model 1, Blue is model 2

```{r}
ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
    geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) + 
  geom_line(aes(x = 1:length(dat$y),
                y = obj2$report()$predictions),
            color = 'blue') + 
  theme_bw() +
  labs(title = 'data (points) and model predictions (line)' )
```

## Model 3: Incorporating uncertainty in the likelihood estimation

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

par$logsdphi = 0
par$logsdalpha = 0
par$logsdy0 = 0

par$alpha = 1 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  sdalpha = exp(logsdalpha)
  sdphi = exp(logsdphi)
  sdy0 = exp(logsdy0)
  
  # assemble model    
  jnll = 0 # init jnll
  
  jnll = jnll - dnorm(alpha, sdalpha, log = TRUE)
  ## from phi
  jnll = jnll - dnorm(phi, sdphi, log = TRUE)
  ## from y0
  jnll = jnll - dnorm(y0, sdy0, log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj3 = MakeADFun(jnll, par)
fit3 = nlminb(obj3$par, obj3$fn, obj3$gr)

sdr3 = sdreport(obj3)
pl3 = as.list(sdr3,"Est")
plsd3 = as.list(sdr3,"Std")
```

```{r}
#| output: asis
summary(sdr3) %>% as.data.frame() %>% xtable %>% print(comment = F)
```

```{r}
# Calculate AIC
logLik_value3 = -fit3$objective
k = length(fit3$par)
aic_value3 = 2 * k - 2 * logLik_value3

# Calculate WAIC
log_lik3 = numeric(length(dat$Y))

# Vector of predicted means
meanvec3 = c(pl3$y0)
  for(i in 2:length(dat$y)){
    meanvec3[i] = tanh(pl3$atanh_phi)*meanvec3[i-1] + pl3$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl3$logsdY)

for(i in 1:length(dat$y)){
  log_lik3[i] = dnorm(dat$y[i], meanvec3[i], sdy, log=TRUE)
}

lppd3 = sum(log(mean(exp(log_lik3))))
p_waic3 = sum(var(log_lik3))
waic3 = -2 * (lppd3 - p_waic3)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)'),
                  AIC = c(aic_value, aic_value2, aic_value3), 
                  WAIC = c(waic, waic2, waic3))) %>% print(comment = F)
```

Black is model 1, Blue is model 2, Green is model 3

```{r}
ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
    geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) + 
  geom_line(aes(x = 1:length(dat$y),
                y = obj2$report()$predictions),
            color = 'blue') + 
  geom_line(aes(x = 1:length(dat$y),
                y = obj3$report()$predictions),
            color = 'green') +
  theme_bw() +
  labs(title = 'data (points) and model predictions (line)' )
```

# Second act: I did come to a SEM course, after all

The predictions obtained in the previous models should be considered with care. After all, they look good on paper even with the poor convergence in Model 3. Furthermore, even if the best fit was a great description of the catch dynamic, its usefulness is very limited. An AR(1) model allows for very little inference, with no viable interpretation or actionability on $\phi$. It's predictive power is also yet to be determined, since no testing was performed on unseen data.

It makes sense, then, to turn our attention to space state models as an alternative, since they would allow us to incorpoate unobserved components of the fishery dynamics and estimate useful parameters for fishery management.

## Model 1: True catch as a latent variable

Following up on the previous idea, the instructors proposed the notion of the true catch being a latent variable whose process equation could be described by an **AR(p)** time series, and the observed catch $C_t$ as a result of the failure to realize or observe the maximum potential catch.

$$
\begin{cases}
  Process: X_t = \phi X_{t-1} + \epsilon_t \\
  Observation: C_t = \alpha X_t + \dots
\end{cases}
$$

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

par$alpha = 1 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj4 = MakeADFun(jnll, par)
fit4 = nlminb(obj4$par, obj4$fn, obj4$gr)

sdr4 = sdreport(obj4)
pl4 = as.list(sdr4,"Est")
plsd4 = as.list(sdr4,"Std")
```

```{r}
#| output: asis
summary(sdr2) %>% as.data.frame() %>% xtable %>% print(comment = F)
```

## Model 2: The Schaefer Surplus Production Model

Let $B_t$ be the stock biomass at time $t$; The most simplistic Schaefer formulation states that, if cephalopod stocks followed the usual equilibrium assumptions, then

$$
B_{t+1} = B_t +r \cdot B_t \left(1 - \frac{B_t}{K} \right) - C_t
$$

We will take a first pass at inference on $B_t$ by treating it as a latent variable and using $C_t$ as the observation sequence. The link between $B_t$ and $C_t$ is given by

$$
\frac{C_t}{E_t} = q \cdot B_t
$$

which means the likelihood function for $B_t$ is derived from its distribution being:

$$
B_t \sim N(q \cdot \frac{C_t}{E_t}, \sigma_{B_t})
$$

for this to work, we need to estimate $q$, $K$, $r$, $B_0$ as well as every point for $B_t$. Fun times! At this time, we will not attempt to directly model parameter uncertainty nor the process error, because life is already too hard as it is.

```{r}
#| eval: true
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort
# dat$Ct = as.vector(iotc$Catch)
# dat$Et = as.vector(iotc$Effort)

# initialize parameter list with initial estimates
par = list()
par$Bt = as.vector(rep(10,length(dat$Ct)))# init empty vector
par$meanBt = 100000

par$q = 0.5
par$r = 0.5
par$K = 4*max(dat$Ct)
par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdBt = 1
par$logsdCt = 1
par$logsdB0 = 1000
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  sdBt = exp(logsdBt)
  sdCt = exp(logsdCt)
  sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
  
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  jnll = jnll - dexp(r,1, log = TRUE)
  jnll = jnll - dexp(q,0.1, log = TRUE)
  jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  ## From the observation equation
 
  
  ## From the process equation
  # Initial values
  Bt[1] = B0
  for(i in 2:length(Ct)){
    Bt[i] = Bt[i-1] + r * Bt[i-1]*(1-(Bt[i-1]/K)) - Ct[i-1]}
  
  
  ## estimation
  
    for(i in 2:length(Ct)){
    predBt = Bt[i-1] + r * Bt[i-1]*(1-(Bt[i-1]/K)) - Ct[i-1]}
    jnll = jnll - dnorm(Bt[i], predBt, sdBt, log=TRUE)
    
  
  
  catch_predictions = c()
  for(i in 1:length(Ct)){
   Ct_hat = q*Bt[i]*Et[i]
   jnll = jnll -dnorm(Ct[i], Ct_hat, sdCt, log = TRUE)
   catch_predictions[i] = Ct_hat
  }
   
  REPORT(predBt)
  REPORT(catch_predictions)
  REPORT(B0)
  REPORT(Bt)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj4 = MakeADFun(jnll, par)
fit4 = nlminb(obj4$par, obj4$fn, obj4$gr)

sdr4 = sdreport(obj4)
pl4 = as.list(sdr4,"Est")
plsd4 = as.list(sdr4,"Std")

# sdr4
obj4$report()$Bt
plot(obj4$report()$Bt)
```

## Model 3: A Surplus Production Model with the catch included as an autorregressive term

This variation was proposed by the instructors with the explicit goal of limiting the impact of the catch in the existing biomass. $C_t$ is shifted from an additive term to a multiplicative one, with the addition of a scaling coefficient $\alpha$, to keep its impact grounded between 0 and 1.

$$
B_{t+1} = B_t +r \cdot B_t \left(1 -\frac{B_t}{K} \right) \cdot \alpha C_t + \epsilon_t
$$

If we combine $\alpha$ and $r$ into a single constant $\alpha$, which could be seen as rescaled natural intrinsic growth, then we could arrange it as

$$
B_{t+1} = B_t \cdot (1 + \alpha ) \cdot \left(1 - \frac{B_t}{K} \right) \cdot C_t + \epsilon_t
$$

# Third act: The mask falls, and the AIC rises

## Model 1: GDM, as seen on CatDyn

# Parameters to be estimated

-   $\alpha$ is the abundance response

-   $\beta$ is the effort response

Both allow non-linearity for $E_t$ and $N_t$;

-   $k$ is a scaling factor

-   $M$ is the natural mortality (with $m = e^{-\frac{M}{2}}$)

-   $N_0$ is the initial abundance of the stock at $t_0$

# Formulation, step by step

Consider the condensed formulation of this model:

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_1}
C_t = kE_t^\alpha N_t^\beta = m kE_t^\alpha f_t(M,N_0, C_{i<t}, R, S) \\
\textrm{with} \quad m = e^{-\frac{M}{2}}
    \end{aligned}
\end{equation}

In order to incorporate this model in RTBM, we will have to break down the abundance estimate ($N_t^\beta$) into chunks that are easier to process. First we present the complete formulation and then we'll break down the chunks of $f(t)$ *ie*, everything that is raised to $\beta$.

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
C_t = kE_t^\alpha  m \left( N_0 e^{-Mt} -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] + \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)} - \sum_{l=1}^{l=v} J_l S_l e^{-M(t-\upsilon_l} \right) ^ \beta
    \end{aligned}
\end{equation}

```{r}
N0 = 1000000
x = 1:348
M = 0.05
```

## Chunk 1: Exponential Population Growth

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch1}
N_0 e^{-Mt}
    \end{aligned}
\end{equation}

This is simply unconstrained population growth, modulated by the natural mortality $M$. Tentative approach:

```{r}
res_1 = c()
# chunk 1
for(tt in 1:length(x)){
ch1 = N0 * exp(-M*tt)

res_1[tt] = ch1
}

  ggplot() + 
  geom_line(aes(x = x,
                y = res_1)) + 
  theme_bw() + 
  labs(title = 'chunk 1')
```

## Chunk 2: Catch carry over

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch2}
 -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] 
    \end{aligned}
\end{equation}

This section incorporates the expected value for the catch in each year, with decay from natural mortality that intensifies the further you are from that year. It is explicitly recursive, unlike chunk 1, and therefore the code must account for that correctly. Specifically, $C_i$ accounts for the *entire* estimate and therefore this chunk can't be correctly computed without the remaining chunks

```{r}
res_2 = c()
# chunk 1
c_i = c()
for(tt in 1:length(x)){
  for(i in 1:(tt-1)){
    catch = ifelse(tt == 1, res_1[1], c_i[tt-1])
    c_i[i] = catch^-M*(tt-i-1)}
  res_2[tt] = sum(c_i)*exp(-M*tt)
}

  ggplot() + 
  geom_line(aes(x = x,
                y = res_2)) + 
  theme_bw() + 
  labs(title = 'chunk 2')
```

## Chunk 3: Recruitment pulse input

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
    \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)}
    \end{aligned}
\end{equation}

As described before, we require an index vector that is correctly specified. There is no documentation on how to handle multiple pulses in the same fishing season.

## Model 2: What if it rained on Model 1

# Conclusion and Final Remarks

# Addendum: Some notes on RTMB running procedures:

*  Parameters can be initialized without explicit assumptions on their prior distribution. Doing so will have the prior being assumed to be uniform:

```{r}
#| eval: false

par$some_p = 0
```

*  In order to explicitly set a prior distribution, we need to incorporate it in the likelihood estimation. Here I'm setting the prior distribution for *some_p*,

```{r}
#| eval: false

par$some_p = 0

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## Prior distributions
  jnll = jnll - dnorm(some_p, 0, 1000, log = TRUE)
  (...)
  }
```

-   Include deviation parameters

-   Impose bounds like log and logit

* The optimization in TMB is performed over unconstrained real numbers. It is a good practice to ensure that parameter estimates are kept within their realistic bounds by applying a transformation:
    
    
    Input the transformed value in the **parameter** section

    Reverse the transformation inside the likelihood constructor function

    Optimize the transformed, unconstrained parameter

    Reverse the transformation

Here are some useful examples:

#### Log transformation: $p \in [0, + \infty]$

```{r}
#| eval: false

par$log_p = log(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(log_p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

   
#### Logit transformation: $p \in [0,1]$

```{r}
#| eval: false

par$logit_p = log(p/(1-p))

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(logit_p)/(1+exp(logit_p))
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```    

#### Tanh transformation: $p \in [-1,1]$

```{r}
#| eval: false

par$atanh_p = atanh(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = tanh(p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```  

-   Do simulations like Anders
