---
title: "Custom SEM"
format:
   pdf:
       include-in-header:
           - text: |
                \usepackage{todonotes}
editor: visual
---

# Setup

```{r}
#| message: false
#| warning: false

# install.packages('TMB', type = 'source', lib = .libPaths()[2]) 
library('RTMB', lib.loc = .libPaths()[1])
library('tidyverse')
library(gridExtra)
library(wesanderson)
library(xtable)
library(CatDyn)
library(tmbstan)
library(shinystan)

load("data/df_effort_m_mbw_otb.Rdata")
# sample data for testing purposes
# iotc = read.csv('data/IOTC-DATASETS-2025-03-13-CELongline.csv')

# iotc =
# iotc %>% filter(Fleet == 'JPN    ') %>%
#   filter(!is.na(YFT.NO)) %>% 
#   group_by(Year, MonthStart) %>% 
#   summarise(Effort = sum(Effort, na.rm =T),
#             Catch = sum(YFT.NO, na.rm = T ) *0.01955034)

precip = c(-16.67,
           115.33, 245.93, 193.93, -229.47,-76.47,
           250.33, 177.73, 115.63, 90.13, -299.57,
           -338.37,97.18, -317.67, -198.74,-14.07,
           221.67, -90.97, -205.97, 98.03, 256.73,
           -241.87, 150.13, -300.17, 98.43, -85.87,
           -94.67,-156.87,-43.87,-106.37) + 
  735.8 + 105.7
anos = 1994:2023

mod_aux = lm(df_effort$catch ~ df_effort$effort)

df_effort = 
  df_effort %>% 
  mutate(catch = case_when(year_sale == 2005 &
                             month_sale == '09' ~
                             effort * mod_aux$coefficients[2] +
                             mod_aux$coefficients[1],
                           T ~ catch),
         catch_otb = case_when(year_sale == 2005 &
                                 month_sale == '09' ~
                                 effort_otb * mod_aux$coefficients[2] +
                                 mod_aux$coefficients[1],
                               T ~ catch_otb)) %>% 
  mutate(catch_otb = case_when(catch_otb == 0 ~ mean(catch_otb), 
                               T ~ catch_otb),
         effort_otb = case_when(effort_otb  < 150 ~ mean(effort_otb),
                                T ~ effort_otb))

```

# Foreword

# Data description and context

## Catch $C_t$

Catch is obtained via reported landings. It can converted to number of individuals via a mean body weight model (not described here), which is prefered in models such as generalized depletion models (GDMs).

For the purposes of this course, 2 outliers were forcefully removed from the dataset. One pertained to an imposed closure on the fishery, while the other was an abnormally low activity month, with 4 fishing days in the entire fleet and 0 octopus landed. In a serious report on this data these gaps would have to be explained and accounted for, but for simplicity sake here they were replaced by the linear prediction of the series in the first outlier (catch-only) and the mean of the series (abnormal catch and effort).

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y = catch_otb/1000)) + 
  theme_bw() + 
  labs(title = 'nominal catches (tons)')
```

## Effort $E_t$

Since we are dealing with a polyvalent fishery that employs a wide range of fishing gears and operates in vessels that are small enough to be exempt from keeping logbooks, effort data can be estimated at best by attempting to count fishing days in each month. Outliers in this series were dealt with as described earlier.

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y =effort_otb)) + 
  theme_bw() + 
  labs(title = 'Effort (fishing days)')
```

## Catch - Effort Dynamics

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_point(aes(x = effort_otb,
                y = catch_otb)) + 
  theme_bw() + 
  labs(title = 'Catch ~ Effort')
```

```{r}
#| echo: false
modelo = lm(df_effort$catch_otb/df_effort$effort_otb ~ df_effort$effort_otb)

a = modelo$coefficients[["df_effort$effort_otb"]]
b = modelo$coefficients[['(Intercept)']]

df_effort %>% 
ggplot() + 
  geom_point(aes(x = effort_otb,
                 y = catch_otb/effort_otb)) + 
  geom_abline(slope = a, intercept = b, color = 'red') + 
  theme_bw()

novo = df_effort %>% 
  mutate(
  cpue_hat = predict(modelo, newdata = df_effort),
                  Y = cpue_hat * df_effort$effort_otb)

novo %>% 
  ggplot() +
  geom_point(aes(x = effort_otb,
                 y = catch_otb), col = 'red') + 
  geom_point(aes(x = effort_otb,
                 y = Y)) + theme_bw()

```

## Precipitation $P_t$

Just total rainfall in Algarve throughout the series period. Unlike the other data, it has an yearly time step. This was done deliberately, to accommodate the challenge of integrating data with mismatching time steps, something that occurs far too often.

```{r}
#| echo: false
  ggplot() + 
  geom_line(aes(x = anos,
                y = precip)) + 
  theme_bw() + 
  labs(title = 'Yearly precipitation')
```

## Mean body weight

Mean body weight model used to convert catch in weight to numbers. Based on samples collected at auction markets.

```{r}
#| echo: false
df_effort %>% 
  ggplot() +
  # geom_line(aes(x = week,
  #               y = mbw,
  #               group = year_sale)) +
  geom_line(aes(x = month_sale,
                y = mbw_rand,
                group = year_sale,
                color = year_sale)) + 
  # scale_color_manual(values = cores2) +
  # facet_wrap(year_sale ~.) + 
  theme_bw() + 
  theme(legend.position = 'none') +
  labs(y = 'mean body weight (kg)')
```

# First act: I flunked my time series course twice

Mention on how some correlation was observed

## Model 1.1: Baseline

In this model we estimate $phi$ and $C_{0}$ (which the catch at $t = 0$ and not actually part of the catch series, hence why the need for estimation).

$$
C_{t+1} = \phi \cdot C_t  + \epsilon_t
$$

```{r}
#| eval: true
#| results: hide
dat = list()

dat$y = as.vector(df_effort$catch_otb)  # catches

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll

  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0}
    else{predY = phi*y[i-1]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")
```

```{r}
#| output: asis
summary(sdr) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

We can calculate the AIC and wAIC: these values are mighty suspicious

```{r}
# Calculate AIC
logLik_value = -fit$objective
k = length(fit$par)
aic_value = 2 * k - 2 * logLik_value

# Calculate WAIC
log_lik = numeric(length(dat$y))

# Vector of predicted means
meanvec = c(pl$y0)
  for(i in 2:length(dat$y)){
    meanvec[i] = tanh(pl$atanh_phi)*meanvec[i-1]}
    #meanvec[i] = tanh(pl$atanh_phi) %*% t(meanvec[i-1])} 

# Revert transformation of sigma(y)  
sdy = exp(pl$logsdY)

for(i in 1:length(dat$y)){
  log_lik[i] = dnorm(dat$y[i], meanvec[i], sdy, log=TRUE)
}

lppd = sum(log(mean(exp(log_lik))))
p_waic = sum(var(log_lik))
waic = -2 * (lppd - p_waic)
```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = 'AR(1)',
                  AIC = aic_value, 
                  WAIC = waic)) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

```{r}
#| fig-height: 4
#| fig-width: 12

pred1 = c(fit$par['y0'])
  for(i in 2:length(dat$y)){
    pred1[i] = tanh(fit$par['atanh_phi']) * pred1[i-1]+
      rnorm(1, 0, exp(fit$par['logsdY']))
  }

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) +
  geom_line(aes(x = seq_along(dat$y),
                y = pred1),
            linetype = 2) +
  theme_bw() +
  labs(title = 'data (points) and model predictions (line)' )
```

## Model 1.2: With Precipitation

In this version, we added rainfall $P_t$ from the previous year to the model. The series is actually lagged, so any given $P_{1995}$ will actually refer to the year of 1994, *eg*. This was done for ease of integration.

$$
C_{t+1} = \phi \cdot C_t + \alpha\cdot P_t + \epsilon_t
$$

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

par$alpha = 1 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj2 = MakeADFun(jnll, par)
fit2 = nlminb(obj2$par, obj2$fn, obj2$gr)

sdr2 = sdreport(obj2)
pl2 = as.list(sdr2,"Est")
plsd2 = as.list(sdr2,"Std")
```

```{r}
#| output: asis
summary(sdr2) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value2 = -fit2$objective
k = length(fit2$par)
aic_value2 = 2 * k - 2 * logLik_value2

# Calculate WAIC
log_lik2 = numeric(length(dat$y))

# Vector of predicted means
meanvec2 = c(pl2$y0)
  for(i in 2:length(dat$y)){
    meanvec2[i] = tanh(pl2$atanh_phi)*meanvec[i-1] + pl2$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl2$logsdY)

for(i in 1:length(dat$y)){
  log_lik2[i] = dnorm(dat$y[i], meanvec2[i], sdy, log=TRUE)
}

lppd2 = sum(log(mean(exp(log_lik2))))
p_waic2 = sum(var(log_lik2))
waic2 = -2 * (lppd2 - p_waic2)
```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt'),
                  AIC = c(aic_value, aic_value2), 
                  WAIC = c(waic, waic2))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

Black is model 1, Blue is model 2

```{r}
#| fig-height: 8
#| fig-width: 12

grid.arrange(

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) + 
  theme_bw() +
  labs(title = 'data (points) and model 1.1 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj2$report()$predictions),
            color = 'blue') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.2 (line)' ),

ncol =1)
```

## Model 1.3: A explicitly Bayesian approach to the estimation of the previous model

Since we are not defining priors, parameters are assumed to follow a uniform distribution before the estimation process. We can see if the model is improved by specifying the priors. As detailed in the addendum to this report, they are inserted as constants:

\begin{equation}
    \begin{aligned}
& \alpha \sim N(1,0.5) \\
& atanh(\phi) \sim N(0, 0.5) \\
& y_0 \sim N(200000,10000) \\
& \sigma_y \sim N(1,1)
    \end{aligned}
\end{equation}

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()

par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
prior_mean_phi = 0
prior_sd_phi = 0.5

par$logsdY = 1 # We must ensure positive numbers, so we work with logs
prior_mean_logsdy = 1
prior_sd_logsdy = 1

par$y0 = mean(dat$y)
prior_mean_y0 = 200000
prior_sd_y0 = 10000 

par$alpha = 1
prior_mean_alpha = 1
prior_sd_alpha = 0.5

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  ## from alpha
  jnll = jnll - dnorm(alpha,
                      prior_mean_alpha,
                      prior_sd_alpha, 
                      log = TRUE)
  ## from phi
  jnll = jnll - dnorm(phi,
                      prior_mean_phi,
                      prior_sd_phi, 
                      log = TRUE)
  ## from y0
  jnll = jnll - dnorm(y0,
                      prior_mean_y0,
                      prior_sd_y0,
                      log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj3 = MakeADFun(jnll, par)
fit3 = nlminb(obj3$par, obj3$fn, obj3$gr)

sdr3 = sdreport(obj3)
pl3 = as.list(sdr3,"Est")
plsd3 = as.list(sdr3,"Std")
```

```{r}
#| output: asis
summary(sdr3) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value3 = -fit3$objective
k = length(fit3$par)
aic_value3 = 2 * k - 2 * logLik_value3

# Calculate WAIC
log_lik3 = numeric(length(dat$y))

# Vector of predicted means
meanvec3 = c(pl3$y0)
  for(i in 2:length(dat$y)){
    meanvec3[i] = tanh(pl3$atanh_phi)*meanvec3[i-1] + pl3$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl3$logsdY)

for(i in 1:length(dat$y)){
  log_lik3[i] = dnorm(dat$y[i], meanvec3[i], sdy, log=TRUE)
}

lppd3 = sum(log(mean(exp(log_lik3))))
p_waic3 = sum(var(log_lik3))
waic3 = -2 * (lppd3 - p_waic3)
```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt',
                            'AR(1) + Pt (ii)'),
                  AIC = c(aic_value, aic_value2, aic_value3), 
                  WAIC = c(waic, waic2, waic3))) %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

Black is model 1.1, Blue is model 1.2, Green is model 1.3

```{r}
#| fig-height: 12
#| fig-width: 12

grid.arrange(

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) + 
  theme_bw() +
  labs(title = 'data (points) and model 1.1 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj2$report()$predictions),
            color = 'blue') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.2 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj2$report()$predictions),
            color = 'darkgreen') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.2 (line)' ),

ncol =1)
```

### Posterior analysis

We can take this model a step further by taking a closer look at the posteriors:

```{r}
#| results: hide
mcmc3 = tmbstan(obj3,chains=1,iter=1000)
temp =
mcmc3 %>% 
  summary()
```

This output is too wide to be rendered, this needs to be fixed

```{r}
#| eval: false
#| output: asis
temp$summary %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

We can compare the priors and posteriors:

```{r}
#| echo: false
#| fig-cap: "Prior (Blue) and posterior (yellow) distributions."
#| fig-height: 7


  cores = c('Prior' = wes_palette('Zissou1')[1],
            'Posterior' = wes_palette('Zissou1')[4])
grid.arrange(

  #y0: prior = N(par$y0, 10000)
  ggplot(data.frame(x=c(min(prior_mean_y0-3*prior_sd_y0,
                             temp$summary['y0','mean'] - 3* temp$summary['y0','sd']),
                        max(prior_mean_y0-3*prior_sd_y0,
                             temp$summary['y0','mean'] + 3* temp$summary['y0','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x,
                                          mean = prior_mean_y0,
                                          sd = prior_sd_y0),
                   aes(fill = "Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x)dnorm(x,
                                         mean = temp$summary['y0','mean'],
                                         sd = temp$summary['y0','sd']),
                  aes(fill = 'Posterior'),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() + 
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'y0') + 
    scale_fill_manual(values = cores),
  
    #alpha: prior = N(1, 0.5)
    ggplot(data.frame(x=c(min(prior_mean_alpha-3*prior_sd_alpha,
                             temp$summary['alpha','mean'] - 3* temp$summary['alpha','sd']),
                        max(prior_mean_alpha-3*prior_sd_alpha,
                             temp$summary['alpha','mean'] + 3* temp$summary['alpha','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_alpha,
                                          sd = prior_sd_alpha),
                   aes(fill="Prior"),
                   linewidth =1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['alpha','mean'],
                                         sd = temp$summary['alpha','sd']),
                  aes(fill="Posterior"),
                  linewidth =1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() +
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'alpha') + 
    scale_fill_manual(values = cores),
  
      #phi: prior = N(0, 0.5)
    ggplot(data.frame(x=c(min(prior_mean_phi-3*prior_sd_phi,
                             temp$summary['atanh_phi','mean'] - 3* temp$summary['atanh_phi','sd']),
                        max(prior_mean_phi-3*prior_sd_phi,
                             temp$summary['atanh_phi','mean'] + 3* temp$summary['atanh_phi','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_phi,
                                          sd = prior_sd_phi),
                   aes(fill="Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['atanh_phi','mean'],
                                         sd = temp$summary['atanh_phi','sd']),
                  aes(fill="Posterior"),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() +
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'atanh(phi)') + 
    scale_fill_manual(values = cores),
  
  
      #logsdy: prior = N(1, 1)
    ggplot(data.frame(x=c(min(prior_mean_logsdy-3*prior_sd_logsdy,
                             temp$summary['logsdY','mean'] - 3* temp$summary['logsdY','sd']),
                        max(prior_mean_logsdy-3*prior_sd_logsdy,
                             temp$summary['logsdY','mean'] + 3* temp$summary['logsdY','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_logsdy,
                                          sd = prior_sd_logsdy),
                   aes(fill="Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['logsdY','mean'],
                                         sd = temp$summary['logsdY','sd']),
                  aes(fill="Posterior"),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() + 
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'log(sd_y)') + 
    scale_fill_manual(values = cores),
  
  ncol=2)
```

In an interactive session, shinystan would also have been an option:

```{r}
#| eval: false
# library(shinystan)
launch_shinystan(mcmc3)
```

# Second act: I did come to a SEM course, after all

The predictions obtained in the previous models should be considered with care. After all, they look good on paper even with the poor convergence in Model 3. Furthermore, even if the best fit was a great description of the catch dynamic, its usefulness is very limited. An AR(1) model allows for very little inference, with no viable interpretation or actionability on $\phi$. It's predictive power is also yet to be determined, since no testing was performed on unseen data.

It makes sense, then, to turn our attention to space state models as an alternative, since they would allow us to incorpoate unobserved components of the fishery dynamics and estimate useful parameters for fishery management.

## Model 2.1: True catch as a latent variable

Following up on the previous idea, the instructors proposed the notion of the true catch being a latent variable whose process equation could be described by an **AR(p)** time series, and the observed catch $C_t$ as a result of the failure to realize or observe the maximum potential catch.

$$
\begin{cases}
  Process: X_t = \phi X_{t-1} + \epsilon_t \\
  Observation: C_t =  f(X_t) + \dots
\end{cases}
$$

Taking some cues from a model we will discuss later, such a model could follow the formulation:

$$
C_t = X_t^ \alpha \cdot E_t ^\beta + \epsilon_t
$$

```{r}
#| results: hide
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$eff = as.vector(df_effort$effort_otb) # effort

# initialize parameter list with initial estimates
par = list()
par$X = rep(1, length(dat$y))

par$logsd_X = 1 # We must ensure positive numbers, so we work with logs
par$logsd_C = 1 # We must ensure positive numbers, so we work with logs

par$alpha = 1
par$beta = 1
par$atanh_phi = atanh(0)

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  
  phi = tanh(atanh_phi)
   # fix the transformation, for the estimation
  sd_X = exp(logsd_X)
  sd_C = exp(logsd_C)
  
  # assemble model    
  jnll = 0 # init jnll
  
  # from the latent process
  
   # prior on X0
   jnll = jnll - dnorm(X[1], max(y), sd_X, log = T) 

  # latent = c(x0)
  for(j in 2:length(dat$y)){
    predX = X[j-1]*phi
    jnll = jnll - dnorm(X[j], predX, sd_X, log =T)
  }
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
  predC = (eff[i]^alpha) * (X[i]^beta)
  jnll = jnll - dnorm(y[i], predC, sd_C, log = T)
  predictions[i] = predC
  }
  
  REPORT(predictions)
  
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj4 = MakeADFun(jnll, par, random = 'X')
fit4 = nlminb(obj4$par, obj4$fn, obj4$gr)

sdr4 = sdreport(obj4)
pl4 = as.list(sdr4,"Est")
plsd4 = as.list(sdr4,"Std")
```

```{r}
#| output: asis
summary(sdr4) %>% 
  as.data.frame() %>% 
  slice(-c(1:348)) %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value4 = -fit4$objective
k = length(fit4$par)
aic_value4 = 2 * k - 2 * logLik_value4

# Calculate WAIC
log_lik4 = numeric(length(dat$y))

# Vector of predicted means
meanvec4 = c()
  for(i in 1:length(dat$y)){
    meanvec4[i] = (dat$eff[i]^pl4$alpha) * (dat$y[i] ^ pl4$beta)}
  
# Revert transformation of sigma(y)  
sdy = exp(pl4$logsd_X)

for(i in 1:length(dat$y)){
  log_lik4[i] = dnorm(dat$y[i], meanvec4[i], sdy, log=TRUE)
}

lppd4 = sum(log(mean(exp(log_lik4))))
p_waic4 = sum(var(log_lik4))
waic4 = -2 * (lppd4 - p_waic4)
```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)', ' $X_t = f(C_t)$'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4), 
                  WAIC = c(waic, waic2, waic3, waic4))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

For these models, we could try a different approach for the plots by extracting the latent variable estimates from the *sdr* object where we stored the report:

TODO: predicted catch with model parameters

```{r}
#| fig-width: 12
#| fig-height: 12


grid.arrange(
  ggplot() + 
  geom_line(aes(x = 1:length(dat$y),
                y = obj4$report()$predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = (summary(sdr4, "random")[,'Estimate'] ^
                        fit4$par['beta']) *
                       (dat$eff^fit4$par['alpha']) + 
                  rnorm(length(dat$y),
                               0,
                               exp(fit4$par['logsd_C']))) ,
            color = 'purple', linetype = 2) +
  theme_bw() +
  labs(title = 'data (points) and catch predictions (line)'),
ggplot() + 
  geom_line(aes(x = 1:length(dat$y),
                y = obj4$report()$predictions),
            color = 'red') +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),
ggplot() + 
  geom_line(aes(x = 1:length(dat$y),
                y = summary(sdr4, "random")[,'Estimate']),
            color = 'purple') +
  geom_ribbon(aes(x = 1:length(dat$y),
                ymax = summary(sdr4, "random")[,'Estimate'] + 
                  2 * summary(sdr4, "random")[,'Std. Error'],
                ymin = summary(sdr4, "random")[,'Estimate'] - 
                  2 * summary(sdr4, "random")[,'Std. Error']),
            fill = 'purple', linetype = 2, alpha = 0.3) +
  theme_bw() +
   labs(title = 'Potential catch estimates (X(t)'),
ggplot() +
  geom_line(aes(x = 1:length(dat$y),
                y = obj4$report()$predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = summary(sdr4, "random")[,'Estimate']),
            color = 'purple') +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),

  ncol=1)
```

## Model 2.2: The Schaefer Surplus Production Model

Let $B_t$ be the stock biomass at time $t$; The most simplistic Schaefer formulation states that, if cephalopod stocks followed the usual equilibrium assumptions, then

$$
B_{t+1} = B_t +r \cdot B_t \left(1 - \frac{B_t}{K} \right) - C_t
$$

We will take a first pass at inference on $B_t$ by treating it as a latent variable and using $C_t$ as the observation sequence. The link between $B_t$ and $C_t$ is given by

$$
\frac{C_t}{E_t} = q \cdot B_t
$$

which means the likelihood function for $B_t$ is derived from its distribution being:

$$
B_t \sim N(q \cdot \frac{C_t}{E_t}, \sigma_{B_t})
$$

for this to work, we need to estimate $q$, $K$, $r$, $B_0$ as well as every point for $B_t$. Fun times! At this time, we will not attempt to directly model parameter uncertainty nor the process error, because life is already too hard as it is.

```{r}
#| results: hide
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort

# initialize parameter list with initial estimates
par = list()
par$Bt = as.vector(rep(10000000,length(dat$Ct)))# init empty vector
# par$meanBt = 100000

par$q = 0.5
par$r = 0.5
par$K = 4*max(dat$Ct)
# par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdBt = 1
par$logsdCt = 1
# par$logsdB0 = 1000
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  sdBt = exp(logsdBt)
  sdCt = exp(logsdCt)
  # sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  jnll = jnll - dexp(r,1, log = TRUE)
  jnll = jnll - dexp(q,0.1, log = TRUE)
  jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  
  # prior on B0
  jnll = jnll - dnorm(Bt[1], K/2, 1000, log = TRUE)
  
  ## From the observation equation
 
  ## From the process equation
  # Initial values
  for(i in 2:length(Ct)){
    predBt = Bt[i-1] + r * Bt[i-1]*(1-(Bt[i-1]/K)) - Ct[i-1]
    jnll = jnll - dnorm(Bt[i], predBt, sdBt, log=TRUE)}
  
 
  catch_predictions = c()
  for(i in 1:length(Ct)){
   Ct_hat = q*Bt[i]*Et[i]
   jnll = jnll -dnorm(Ct[i], Ct_hat, sdCt, log = TRUE)
   catch_predictions[i] = Ct_hat
  }
   
  REPORT(predBt)
  REPORT(catch_predictions)
  # REPORT(B0)
  REPORT(Bt)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj5 = MakeADFun(jnll, par, random = 'Bt')
fit5 = nlminb(obj5$par, obj5$fn, obj5$gr)

sdr5 = sdreport(obj5)
pl5 = as.list(sdr5,"Est")
plsd5 = as.list(sdr5,"Std")
```

```{r}
#| output: asis
summary(sdr4) %>% 
  as.data.frame() %>% 
  slice(-c(1:348)) %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

\todo[inline]{Update the meanvec and sd}

```{r}
# Calculate AIC
logLik_value5 = -fit5$objective
k = length(fit5$par)
aic_value5 = 2 * k - 2 * logLik_value5

# Calculate WAIC
log_lik5 = numeric(length(dat$y))

# Vector of predicted means
meanvec5 = c()
  for(i in 1:length(dat$Ct)){
    meanvec5[i] = fit5$par['q'] * 
      summary(sdr5, "random")[,'Estimate'][i] *
      dat$Et[i] }
  
# Revert transformation of sigma(y)  
sdy = exp(pl5$logsdCt)

for(i in 1:length(dat$y)){
  log_lik5[i] = dnorm(dat$Ct[i], meanvec5[i], sdy, log=TRUE)
}

lppd5 = sum(log(mean(exp(log_lik5))))
p_waic5 = sum(var(log_lik5))
waic5 = -2 * (lppd5 - p_waic5)

```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)',
                            ' $X_t = f(C_t)$', 'SPM'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4, aic_value5), 
                  WAIC = c(waic, waic2, waic3, waic4, waic5))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

```{r}
#| fig-width: 12
#| fig-height: 12


grid.arrange(
  ggplot() + 
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj5$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = meanvec5 + 
                  rnorm(length(dat$Ct),
                               0,
                               exp(fit5$par['logsdCt']))) ,
            color = 'purple', linetype = 2) +
  theme_bw() +
  labs(title = 'data (points) and catch predictions (line)'),
ggplot() + 
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj5$report()$catch_predictions),
            color = 'red') +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),
ggplot() + 
  geom_line(aes(x = 1:length(dat$Ct),
                y = summary(sdr5, "random")[,'Estimate']),
            color = 'purple') +
  geom_ribbon(aes(x = 1:length(dat$Ct),
                ymax = summary(sdr5, "random")[,'Estimate'] + 
                  2 * summary(sdr5, "random")[,'Std. Error'],
                ymin = summary(sdr5, "random")[,'Estimate'] - 
                  2 * summary(sdr5, "random")[,'Std. Error']),
            fill = 'purple', linetype = 2, alpha = 0.3) +
  theme_bw() +
   labs(title = 'Potential catch estimates (X(t)'),
ggplot() +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj5$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = summary(sdr5, "random")[,'Estimate']),
            color = 'purple') +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),

  ncol=1)
```

## Model 2.3: A Surplus Production Model with the catch included as an autorregressive term

This variation was proposed by the instructors with the explicit goal of limiting the impact of the catch in the existing biomass. $C_t$ is shifted from an additive term to a multiplicative one, with the addition of a scaling coefficient $\alpha$, to keep its impact grounded between 0 and 1.

$$
B_{t+1} = B_t +r \cdot B_t \left(1 -\frac{B_t}{K} \right) \cdot \alpha C_t + \epsilon_t
$$

If we combine $\alpha$ and $r$ into a single constant $\alpha$, which could be seen as rescaled natural intrinsic growth, then we could arrange it as

$$
B_{t+1} = B_t \cdot (1 + \alpha ) \cdot \left(1 - \frac{B_t}{K} \right) \cdot C_t + \epsilon_t
$$

# Third act: The mask falls, and the AIC rises

## Model 1: GDM, as seen on CatDyn

# Parameters to be estimated

-   $\alpha$ is the abundance response

-   $\beta$ is the effort response

Both allow non-linearity for $E_t$ and $N_t$;

-   $k$ is a scaling factor

-   $M$ is the natural mortality (with $m = e^{-\frac{M}{2}}$)

-   $N_0$ is the initial abundance of the stock at $t_0$

# Formulation, step by step

Consider the condensed formulation of this model:

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_1}
C_t = kE_t^\alpha N_t^\beta = m kE_t^\alpha f_t(M,N_0, C_{i<t}, R, S) \\
\textrm{with} \quad m = e^{-\frac{M}{2}}
    \end{aligned}
\end{equation}

In order to incorporate this model in RTBM, we will have to break down the abundance estimate ($N_t^\beta$) into chunks that are easier to process. First we present the complete formulation and then we'll break down the chunks of $f(t)$ *ie*, everything that is raised to $\beta$.

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
C_t = kE_t^\alpha  m \left( N_0 e^{-Mt} -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] + \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)} - \sum_{l=1}^{l=v} J_l S_l e^{-M(t-\upsilon_l} \right) ^ \beta
    \end{aligned}
\end{equation}

```{r}
N0 = 1000000
x = 1:348
M = 0.05
```

## Chunk 1: Exponential Population Growth

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch1}
N_0 e^{-Mt}
    \end{aligned}
\end{equation}

This is simply unconstrained population growth, modulated by the natural mortality $M$. Tentative approach:

```{r}
res_1 = c()
# chunk 1
for(tt in 1:length(x)){
ch1 = N0 * exp(-M*tt)

res_1[tt] = ch1
}

  ggplot() + 
  geom_line(aes(x = x,
                y = res_1)) + 
  theme_bw() + 
  labs(title = 'chunk 1')
```

## Chunk 2: Catch carry over

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch2}
 -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] 
    \end{aligned}
\end{equation}

This section incorporates the expected value for the catch in each year, with decay from natural mortality that intensifies the further you are from that year. It is explicitly recursive, unlike chunk 1, and therefore the code must account for that correctly. Specifically, $C_i$ accounts for the *entire* estimate and therefore this chunk can't be correctly computed without the remaining chunks

```{r}
res_2 = c()
# chunk 1
c_i = c()
for(tt in 1:length(x)){
  for(i in 1:(tt-1)){
    catch = ifelse(tt == 1, res_1[1], c_i[tt-1])
    c_i[i] = catch^-M*(tt-i-1)}
  res_2[tt] = sum(c_i)*exp(-M*tt)
}

  ggplot() + 
  geom_line(aes(x = x,
                y = res_2)) + 
  theme_bw() + 
  labs(title = 'chunk 2')
```

## Chunk 3: Recruitment pulse input

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
    \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)}
    \end{aligned}
\end{equation}

As described before, we require an index vector that is correctly specified. There is no documentation on how to handle multiple pulses in the same fishing season.

## Model 2: What if it rained on Model 1

# Conclusion and Final Remarks

# Addendum: Some notes on RTMB running procedures:

-   Parameters can be initialized without explicit assumptions on their prior distribution. Doing so will have the prior being assumed to be uniform:

```{r}
#| eval: false

par$p = 0
```

-   In order to explicitly set a prior distribution, we need to incorporate it in the likelihood estimation. Here I'm setting the prior distribution for $p$:

$$
p \sim N(\mu_p, \sigma_p)
$$ these priors are incorporated in the likelihood constructor function as **constants**, not *parameters to be estimated*:

```{r}
#| eval: false

par$p = 0

# these are priors, not parameters, so we write them like this:
prior_mean_p = 0
prior_sigma_p = 10

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## Prior distributions
  jnll = jnll - dnorm(p, prior_mean_p, prior_sd_p, log = TRUE)
  (...)
  }
```

-   The optimization in TMB is performed over unconstrained real numbers. It is a good practice to ensure that parameter estimates are kept within their realistic bounds by applying a transformation:

    Input the transformed value in the **parameter** section

    Reverse the transformation inside the likelihood constructor function

    Optimize the transformed, unconstrained parameter

    Reverse the transformation

Here are some useful examples:

#### Log transformation: $p \in [0, + \infty]$

```{r}
#| eval: false

par$log_p = log(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(log_p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

#### Logit transformation: $p \in [0,1]$

```{r}
#| eval: false

par$logit_p = log(p/(1-p))

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(logit_p)/(1+exp(logit_p))
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

#### Tanh transformation: $p \in [-1,1]$

```{r}
#| eval: false

par$atanh_p = atanh(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = tanh(p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

-   One can add explicit boundaries for the parameters by adding them as named vectors to the parameter list:

```{r}
# Bounds for parameters in TMB
par$lower = list(r = 0, q = 0.01, K = 100, B0 = 0, logsdBt = -10, logsdCt = -10, logsdB0 = 0)
par$upper = list(r = 5, q = 10, K = 100000, B0 = par$K, logsdBt = 10, logsdCt = 10, logsdB0 = 10000)

```

-   A procedure to test the robustness of the model (as sugested by Anders Nielsen)

```{r}
#| results: hide
subject = obj
jit = replicate(100, 
                nlminb(subject$par+rnorm(length(subject$par),
                                         sd=.25),subject$fn,subject$gr)$par)
```

```{r}
jitplots = list()
for(i in 1: dim(jit)[1]){
  
  cor = colorRampPalette(wes_palette('Zissou1'))(dim(jit)[1])[i]
  # run in case of latent variables to avoid exploding the grOb
  if(dim(jit)[1] > 348 & i <= 348){next}
  jitplots[[length(jitplots)+1]] = 
    ggplot(data = data.frame(y = jit[i,])) + 
    geom_boxplot(aes(y=y), color = cor) + 
    labs(title = row.names(jit)[i],
         y = '') + 
    theme_bw()
   
}

grid.arrange(grobs = jitplots, nrow = 1)

apply(jit-fit$par,1,range)
```
