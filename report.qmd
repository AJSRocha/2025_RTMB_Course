---
title: "Report on the octopus fishery case study examined during the SEM Workshop"
author: Alberto Rocha
format:
   pdf:
       include-in-header:
           - text: |
                \usepackage{todonotes}
editor: visual
---

# Foreword

The dataset that I brought to this workshop and will be using in this report consists of real data from the polyvalent fishery of octopus that is occurs in the Southern region of Algarve. Octopus, as most cephalopods, are somewhat infamous in fish stock assessment due to the fact that they are not fish and are quite problematic to assess properly due to their unique population dynamics and life cycle.

This particular case study is a problem that is currently being worked on by several experts in the community and I do not expect to pull a rabbit out of this particular hat just by being introduced to SEM. I chose this dataset because I am familiar with it and any new insight will be valuable, but throughout the workshop and the work after I prioritized using it as a *guinea pig* for the tecniques and modelling approaches that were introduced throughout the workshop, even at the expense of the optimal approach or sometimes common sense. Poor decisions and wrong formulations were sometimes pursued just to ensure I learned how to implement them, foregoing progress in the dataset itself. The decision of including a yearly series of precipitation as a proxy for the incorporation of environmental data is an example of this thought process.

I must once again thank the instructors for their patience in the face of what must have come across as stubborness, lack of common sense and poor decision making. Cheers!

The source for this document can be found in

\url{https://github.com/AJSRocha/2025_RTMB_Course/blob/main/report.qmd}

# Setup

```{r}
#| message: false
#| warning: false

set.seed(1)

# tinytex::parse_install("report.log")
# tinytex::tlmgr_update()

# install.packages('TMB', type = 'source', lib = .libPaths()[2]) 
library('RTMB', lib.loc = .libPaths()[1])
library('tidyverse')
library(gridExtra)
library(wesanderson)
library(xtable)
library(CatDyn)
library(tmbstan)
library(shinystan)

load("data/df_effort_m_mbw_otb.Rdata")

precip = c(-16.67,
           115.33, 245.93, 193.93, -229.47,-76.47,
           250.33, 177.73, 115.63, 90.13, -299.57,
           -338.37,97.18, -317.67, -198.74,-14.07,
           221.67, -90.97, -205.97, 98.03, 256.73,
           -241.87, 150.13, -300.17, 98.43, -85.87,
           -94.67,-156.87,-43.87,-106.37) + 
  735.8 + 105.7

mod_aux = lm(df_effort$catch ~ df_effort$effort)

# Fix a couple of outliers
df_effort = 
  df_effort %>% 
  mutate(catch = case_when(year_sale == 2005 &
                             month_sale == '09' ~
                             effort * mod_aux$coefficients[2] +
                             mod_aux$coefficients[1],
                           T ~ catch),
         catch_otb = case_when(year_sale == 2005 &
                                 month_sale == '09' ~
                                 effort_otb * mod_aux$coefficients[2] +
                                 mod_aux$coefficients[1],
                               T ~ catch_otb)) %>% 
  mutate(catch_otb = case_when(catch_otb == 0 ~ mean(catch_otb), 
                               T ~ catch_otb),
         effort_otb = case_when(effort_otb  < 150 ~ mean(effort_otb),
                                T ~ effort_otb))

```

# Data description and context

In broad strokes, the octopus fishery in southern Portugal is a small scale fishery, mostly performed with passive traps, where the octopi are free to enter and leave at will. Lack of reliable effort measures and auction market dodging are just a few problems that plague data collection on this fishery. The data used in this report covers 29 years, in month time steps, for a total of 348 observations.

## Catch $C_t$

Catch is obtained via reported landings. It can converted to number of individuals via a mean body weight model (fitted with spline regression, not described here), which is prefered in models such as generalized depletion models (GDMs).

For the purposes of this course, 2 outliers were forcefully removed from the dataset. One pertained to an imposed closure on the fishery, while the other was an abnormally low activity month, with 4 fishing days in the entire fleet and 0 octopus landed. Were this not an exercise, these gaps would have to be explained and accounted for, but for simplicity sake here they were replaced by the linear prediction of the series in the first outlier (catch-only) and the mean of the series (abnormal catch and effort).

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y = catch_otb/1000)) + 
  theme_bw() + 
  labs(title = 'nominal catches (tons)')
```

## Effort $E_t$

Since we are dealing with a polyvalent fishery that employs a wide range of fishing gears and operates in vessels that are small enough to be exempt from keeping logbooks, effort data can be estimated at best by attempting to count fishing days in each month. Outliers in this series were dealt with as described earlier.

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y =effort_otb)) + 
  theme_bw() + 
  labs(title = 'Effort (fishing days)')
```

## Catch - Effort Dynamics

A key point in stock assessment is characterizing the dynamic between catch and effort. Typically it should be expected that after a given point catch per unit of effort starts to diminish, a phenomenon known as effort saturation. This often occurs because abundance becomes a limiting factor, and this shift in response to effort is crucial to understand the fishery. Stock assessment models often require a measure of contrast in the data, where different levels of effort and response are observed in order to characterize this relationship.

In many cases, factors such as inacuracies in the data, shifts in fishing capacity, efficiency and preference patterns or non-fishing-related changes in biomass can obscur this relationship. There is a consensus on the need of standardizing the *catch-per-unit-of-effort* (**CPUE**) indexes in order to mitigate the influence of these factors in longer time series.

In this particular series, the catch-effort relationship appears to be fairly linear without the expected decay and/or inverse exponential shape. This may be an indication that either the data is somewhat unrealistic or, perhaps, the fishing mortality has not been enough to cause a significant enough shift in abundance to be reflected in the *CPUE*.

```{r}
#| echo: false
df_effort %>% 
  ggplot() + 
  geom_point(aes(x = effort_otb,
                y = catch_otb)) + 
  theme_bw() + 
  labs(title = 'Catch ~ Effort')
```

Plotting the linear relationship between *CPUE* and effort in the catch-effort map should result in the typical inverted bell shape, with the peak in the middle representing the point of maximum yield-per-effort. As we can see below, that is not the case at all with our data. Either the observations only cover a small fraction of the catch-effort dynamic or the data is not a good sample of this relationship.

```{r}
#| echo: false
modelo = lm(df_effort$catch_otb/df_effort$effort_otb ~ df_effort$effort_otb)

a = modelo$coefficients[["df_effort$effort_otb"]]
b = modelo$coefficients[['(Intercept)']]

df_effort %>% 
ggplot() + 
  geom_point(aes(x = effort_otb,
                 y = catch_otb/effort_otb)) + 
  geom_abline(slope = a, intercept = b, color = 'red') + 
  theme_bw() +
  labs(x = 'effort', y = 'CPUE')

novo = df_effort %>% 
  mutate(
  cpue_hat = predict(modelo, newdata = df_effort),
                  Y = cpue_hat * df_effort$effort_otb)

novo %>% 
  ggplot() +
  geom_point(aes(x = effort_otb,
                 y = catch_otb), col = 'red') + 
  geom_point(aes(x = effort_otb,
                 y = Y)) + theme_bw() + 
  labs(x = 'effort', y = 'catch')

```

## Precipitation $P_t$

Just total rainfall in Algarve throughout the series period. Unlike the other data, it has an yearly time step. This was done deliberately, to accommodate the challenge of integrating data with mismatching time steps, something that occurs far too often.

```{r}
#| echo: false
  ggplot() + 
  geom_line(aes(x = 1994:2023,
                y = precip)) + 
  theme_bw() + 
  labs(title = 'Yearly precipitation', x = 'year', y = 'precipitation')
```

## Mean body weight

Mean body weight model used to convert catch in weight to numbers. Based on samples collected at auction markets, the model itself was fitted through spline regression.

```{r}
#| echo: false
df_effort %>% 
  ggplot() +
  # geom_line(aes(x = week,
  #               y = mbw,
  #               group = year_sale)) +
  geom_line(aes(x = month_sale,
                y = mbw_rand,
                group = year_sale,
                color = year_sale)) + 
  # scale_color_manual(values = cores2) +
  # facet_wrap(year_sale ~.) + 
  theme_bw() + 
  theme(legend.position = 'none') +
  labs(y = 'mean body weight (kg)', x = 'month')
```

# First act: I flunked my time series course twice

Upon inspecting the catch series ($C_t$), the instructors quickly pointed out that it seems to be an autorregressive process. While the temporal independence of the catches could be a matter of discussion (technically they are independent observations, but they are linked by a latent process), it is a fair call to treat these observations as serially correlated. While such models have limited applicability for our purposes, we will start by fitting an $AR(1)$ model.

## Model 1.1: Baseline

In this model we estimate $\phi$, $\epsilon_t$ and $C_{0}$ (which is the catch at $t = 0$ and not actually part of the catch series, hence why the need for estimation). 

$$
C_{t+1} = \phi \cdot C_t  + \epsilon_t
$$

```{r}
#| eval: true
#| results: hide
dat = list()

dat$y = as.vector(df_effort$catch_otb)  # catches

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll

  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0}
    else{predY = phi*y[i-1]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")
```

```{r}
#| output: asis
summary(sdr) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

We can calculate the AIC and wAIC as such:

```{r}
# Calculate AIC
logLik_value = -fit$objective
k = length(fit$par)
aic_value = 2 * k - 2 * logLik_value

# Calculate WAIC
log_lik = numeric(length(dat$y))

# Vector of predicted means
meanvec = c(pl$y0)
  for(i in 2:length(dat$y)){
    meanvec[i] = tanh(pl$atanh_phi)*meanvec[i-1]}
    #meanvec[i] = tanh(pl$atanh_phi) %*% t(meanvec[i-1])} 

# Revert transformation of sigma(y)  
sdy = exp(pl$logsdY)

for(i in 1:length(dat$y)){
  log_lik[i] = dnorm(dat$y[i], meanvec[i], sdy, log=TRUE)
}

lppd = sum(log(mean(exp(log_lik))))
p_waic = sum(var(log_lik))
waic = -2 * (lppd - p_waic)
```

```{r}
#| output: asis
#| echo: true

xtable(data.frame(Model = 'AR(1)',
                  AIC = aic_value, 
                  WAIC = waic)) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

We got satisfactory model convergence, so we are starting with a working model as a baseline. However, when we plot the predictions from this model we are reminded that the deterministic component in an $AR(1)$ is not sufficient to account for the variance we observe in the data. In our series, in order to match the extreme shifts in catches the estimate for $\epsilon_t$ is inflated to a degree that causes the catch predictions to get deep into negative values.

```{r}
#| fig-height: 4
#| fig-width: 12
#| echo: false

set.seed(1)

pred1 = c(fit$par['y0'])
  for(i in 2:length(dat$y)){
    pred1[i] = rnorm(1,
                     tanh(fit$par['atanh_phi']) * pred1[i-1],
                     exp(fit$par['logsdY']))
  }

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = obj$report()$predictions)) +
  geom_line(aes(x = seq_along(dat$y),
                y = pred1),
            linetype = 2) +
  theme_bw() +
  labs(title = 'data (points) and model predictions (line)' )
```

## Model 1.2: With Precipitation

In this version, we added rainfall $P_t$ from the previous year to the model. The series is actually lagged, so any given $P_{1995}$ will actually refer to the year of 1994, *eg*. This was done for ease of incorporating the series into the model.

$$
C_{t+1} = \phi \cdot C_t + \alpha\cdot P_t + \epsilon_t
$$

```{r}
#| results: hide

dat = list()
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
par$logsdY = 1 # We must ensure positive numbers, so we work with logs
par$y0 = mean(dat$y)

par$alpha = 1 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj2 = MakeADFun(jnll, par)
fit2 = nlminb(obj2$par, obj2$fn, obj2$gr)

sdr2 = sdreport(obj2)
pl2 = as.list(sdr2,"Est")
plsd2 = as.list(sdr2,"Std")
```

```{r}
#| output: asis
#| echo: false

summary(sdr2) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value2 = -fit2$objective
k = length(fit2$par)
aic_value2 = 2 * k - 2 * logLik_value2

# Calculate WAIC
log_lik2 = numeric(length(dat$y))

# Vector of predicted means
meanvec2 = c(pl2$y0)
  for(i in 2:length(dat$y)){
    meanvec2[i] = tanh(pl2$atanh_phi)*meanvec2[i-1] + pl2$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl2$logsdY)

for(i in 1:length(dat$y)){
  log_lik2[i] = dnorm(dat$y[i], meanvec2[i], sdy, log=TRUE)
}

lppd2 = sum(log(mean(exp(log_lik2))))
p_waic2 = sum(var(log_lik2))
waic2 = -2 * (lppd2 - p_waic2)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt'),
                  AIC = c(aic_value, aic_value2), 
                  WAIC = c(waic, waic2))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

Below we can compare both models (Black is model 1, Blue is model 2. We still have dispersion estimates for the parameters and even though the improvements in the *AIC* and *wAIC* are small, the plausibility of the estimates is considerably better.

```{r}
#| fig-height: 8
#| fig-width: 12
#| echo: false

set.seed(1)

pred2 = c(pl2$y0)
for(i in 2:length(dat$y)){
  pred2[i] = rnorm(1,
                   pred2[i-1]*tanh(pl2$atanh_phi) + pl2$alpha*dat$x[i],
                   exp(pl2$logsdY))
}

grid.arrange(

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = pred1)) + 
  theme_bw() +
  labs(title = 'data (points) and model 1.1 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y =pred2),
            color = 'blue') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.2 (line)' ),

ncol =1)
```

## Model 1.3: A explicitly Bayesian approach to the estimation of the previous model

Since we are not defining priors, parameters are assumed to follow a uniform distribution before the estimation process. We can see if the model is improved by specifying the priors. As detailed in the addendum to this report, they are inserted as constants:

\begin{equation}
    \begin{aligned}
& \alpha \sim N(1,0.5) \\
& atanh(\phi) \sim N(0, 0.5) \\
& y_0 \sim N(200000,10000) \\
& \sigma_y \sim N(1,1)
    \end{aligned}
\end{equation}

```{r}
#| results: hide

dat = list()
dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()

par$atanh_phi = atanh(0) #ideally, should be bound between [-1,1]
prior_mean_phi = 0
prior_sd_phi = 0.5

par$logsdY = 1 # We must ensure positive numbers, so we work with logs
prior_mean_logsdy = 1
prior_sd_logsdy = 1

par$y0 = mean(dat$y)
prior_mean_y0 = 200000
prior_sd_y0 = 10000 

par$alpha = 1
prior_mean_alpha = 1
prior_sd_alpha = 0.5

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  # fix the transformation, for the estimation
  phi = tanh(atanh_phi)
  sdY = exp(logsdY)
  
  # assemble model    
  jnll = 0 # init jnll
  ## from alpha
  jnll = jnll - dnorm(alpha,
                      prior_mean_alpha,
                      prior_sd_alpha, 
                      log = TRUE)
  ## from phi
  jnll = jnll - dnorm(phi,
                      prior_mean_phi,
                      prior_sd_phi, 
                      log = TRUE)
  ## from y0
  jnll = jnll - dnorm(y0,
                      prior_mean_y0,
                      prior_sd_y0,
                      log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0 + alpha*x[1]}
    else{predY = phi*y[i-1] + alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj3 = MakeADFun(jnll, par)
fit3 = nlminb(obj3$par, obj3$fn, obj3$gr)

sdr3 = sdreport(obj3)
pl3 = as.list(sdr3,"Est")
plsd3 = as.list(sdr3,"Std")
```

```{r}
#| output: asis
#| echo: false

summary(sdr3) %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value3 = -fit3$objective
k = length(fit3$par)
aic_value3 = 2 * k - 2 * logLik_value3

# Calculate WAIC
log_lik3 = numeric(length(dat$y))

# Vector of predicted means
meanvec3 = c(pl3$y0)
  for(i in 2:length(dat$y)){
    meanvec3[i] = tanh(pl3$atanh_phi)*meanvec3[i-1] + pl3$alpha * dat$x[i]}
  
# Revert transformation of sigma(y)  
sdy = exp(pl3$logsdY)

for(i in 1:length(dat$y)){
  log_lik3[i] = dnorm(dat$y[i], meanvec3[i], sdy, log=TRUE)
}

lppd3 = sum(log(mean(exp(log_lik3))))
p_waic3 = sum(var(log_lik3))
waic3 = -2 * (lppd3 - p_waic3)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt',
                            'AR(1) + Pt (ii)'),
                  AIC = c(aic_value, aic_value2, aic_value3), 
                  WAIC = c(waic, waic2, waic3))) %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

We can see that by both metrics and predictions, specifying the priors did not result in improvements:

\todo[inline]{finish here}

```{r}
#| fig-height: 12
#| fig-width: 12
#| echo: false

set.seed(1)

pred3 = c(pl3$y0)
for(i in 2:length(dat$y)){
  pred3[i] = 
    rnorm(
      1,
      pred3[i-1]*tanh(pl3$atanh_phi) + pl3$alpha*dat$x[i],
      exp(pl3$logsdY))
}

grid.arrange(

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = pred1)) + 
  theme_bw() +
  labs(title = 'data (points) and model 1.1 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y =pred2),
            color = 'blue') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.2 (line)' ),

ggplot() + 
  geom_point(aes(x = 1:length(dat$y),
                 y = dat$y),
            col = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = pred3),
            color = 'darkgreen') + 
  theme_bw() +
  labs(title = 'data (points) and model 1.3 (line)' ),

ncol =1)
```

The predictions of Model 1 and Model 3 are indeed very close to one another:

```{r}
(pred3-pred1) %>% hist()
```

### Posterior analysis

We can take this model a step further by taking a closer look at the posteriors:

```{r}
#| results: hide
#| warning: false
mcmc3 = tmbstan(obj3,chains=1,iter=1000)
temp =
mcmc3 %>% 
  summary()
```

\todo{This output is too wide to be rendered, this needs to be fixed. Also, some warnings in the MCMC chain}

```{r}
#| eval: false
#| output: asis
#| echo: false

temp$summary %>% 
  as.data.frame() %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

We can compare the priors and posteriors:

```{r}
#| echo: false
#| fig-cap: "Prior (Blue) and posterior (yellow) distributions."
#| fig-height: 7


  cores = c('Prior' = wes_palette('Zissou1')[1],
            'Posterior' = wes_palette('Zissou1')[4])
grid.arrange(

  #y0: prior = N(par$y0, 10000)
  ggplot(data.frame(x=c(min(prior_mean_y0-3*prior_sd_y0,
                             temp$summary['y0','mean'] - 3* temp$summary['y0','sd']),
                        max(prior_mean_y0+3*prior_sd_y0,
                             temp$summary['y0','mean'] + 3* temp$summary['y0','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x,
                                          mean = prior_mean_y0,
                                          sd = prior_sd_y0),
                   aes(fill = "Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x)dnorm(x,
                                         mean = temp$summary['y0','mean'],
                                         sd = temp$summary['y0','sd']),
                  aes(fill = 'Posterior'),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() + 
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'y0') + 
    scale_fill_manual(values = cores),
  
    #alpha: prior = N(1, 0.5)
    ggplot(data.frame(x=c(min(prior_mean_alpha-3*prior_sd_alpha,
                             temp$summary['alpha','mean'] - 3* temp$summary['alpha','sd']),
                        max(prior_mean_alpha+3*prior_sd_alpha,
                             temp$summary['alpha','mean'] + 3* temp$summary['alpha','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_alpha,
                                          sd = prior_sd_alpha),
                   aes(fill="Prior"),
                   linewidth =1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['alpha','mean'],
                                         sd = temp$summary['alpha','sd']),
                  aes(fill="Posterior"),
                  linewidth =1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() +
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'alpha') + 
    scale_fill_manual(values = cores),
  
      #phi: prior = N(0, 0.5)
    ggplot(data.frame(x=c(min(prior_mean_phi-3*prior_sd_phi,
                             temp$summary['atanh_phi','mean'] - 3* temp$summary['atanh_phi','sd']),
                        max(prior_mean_phi+3*prior_sd_phi,
                             temp$summary['atanh_phi','mean'] + 3* temp$summary['atanh_phi','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_phi,
                                          sd = prior_sd_phi),
                   aes(fill="Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['atanh_phi','mean'],
                                         sd = temp$summary['atanh_phi','sd']),
                  aes(fill="Posterior"),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() +
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'atanh(phi)') + 
    scale_fill_manual(values = cores),
  
  
      #logsdy: prior = N(1, 1)
    ggplot(data.frame(x=c(min(prior_mean_logsdy-3*prior_sd_logsdy,
                             temp$summary['logsdY','mean'] - 3* temp$summary['logsdY','sd']),
                        max(prior_mean_logsdy+3*prior_sd_logsdy,
                             temp$summary['logsdY','mean'] + 3* temp$summary['logsdY','sd']))), aes(x)) +
     stat_function(fun= function(x) dnorm(x ,mean = prior_mean_logsdy,
                                          sd = prior_sd_logsdy),
                   aes(fill="Prior"),
                   linewidth = 1, linetype = 1, geom = 'area', alpha = 0.3) +
    stat_function(fun= function(x) dnorm(x,
                                         mean = temp$summary['logsdY','mean'],
                                         sd = temp$summary['logsdY','sd']),
                  aes(fill="Posterior"),
                  linewidth = 1, linetype = 2, geom = 'area', alpha = 0.3) + 
    theme_bw() + 
    theme(legend.position = 'none') + 
    labs(colour = '', title = 'log(sd_y)') + 
    scale_fill_manual(values = cores),
  
  ncol=2)
```

In an interactive session, shinystan would also have been an option:

```{r}
#| eval: false
# library(shinystan)
launch_shinystan(mcmc3)
```

# Second act: I did come to a SEM course, after all

The predictions obtained in the previous models should be considered with care. After all, they look good on paper even with the poor convergence in Model 1.3. Furthermore, even if the best fit was a great description of the catch dynamic, its usefulness is very limited. An AR(1) model allows for very little inference, with no viable interpretation or actionability on $\phi$. It's predictive power is also yet to be determined, since no testing was performed on unseen data.

It makes sense, then, to turn our attention to space state models as an alternative, since they would allow us to incorpoate unobserved components of the fishery dynamics and estimate useful parameters for fishery management.

## Model 2.1: True catch as a latent variable

Following up on the previous idea, the instructors proposed the notion of the true catch being a latent variable whose process equation could be described by an **AR(p)** time series, and the observed catch $C_t$ as a result of the failure to realize or observe the maximum potential catch.

$$
\begin{cases}
  Process: X_t = \phi X_{t-1} + \epsilon_t \\
  Observation: C_t =  f(X_t) + \dots
\end{cases}
$$

Taking some cues from a model we will discuss later, such a model could follow the formulation:

$$
C_t = X_t^ \alpha \cdot E_t ^\beta + \epsilon_t
$$

```{r}
#| results: hide
#| warning: false

dat = list()

dat$y = as.vector(df_effort$catch_otb)  # catches
dat$eff = as.vector(df_effort$effort_otb) # effort

# initialize parameter list with initial estimates
par = list()
par$X = rep(1, length(dat$y))

par$logsd_X = 1 # We must ensure positive numbers, so we work with logs
par$logsd_C = 1 # We must ensure positive numbers, so we work with logs

par$alpha = 1
par$beta = 1
par$atanh_phi = atanh(0)

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  
  phi = tanh(atanh_phi)
   # fix the transformation, for the estimation
  sd_X = exp(logsd_X)
  sd_C = exp(logsd_C)
  
  # assemble model    
  jnll = 0 # init jnll
  
  # from the latent process
  
   # prior on X0
   jnll = jnll - dnorm(X[1], max(y), sd_X, log = T) 

  # latent = c(x0)
  for(j in 2:length(dat$y)){
    predX = X[j-1]*phi
    jnll = jnll - dnorm(X[j], predX, sd_X, log =T)
  }
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
  predC = (eff[i]^alpha) * (X[i]^beta)
  jnll = jnll - dnorm(y[i], predC, sd_C, log = T)
  predictions[i] = predC
  }
  
  REPORT(predictions)
  
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj4 = MakeADFun(jnll, par, random = 'X')
fit4 = nlminb(obj4$par, obj4$fn, obj4$gr)

sdr4 = sdreport(obj4)
pl4 = as.list(sdr4,"Est")
plsd4 = as.list(sdr4,"Std")
```

\todo[inline]{this chunk gave some NaN warnings which were supressed}

```{r}
#| echo: false
#| warning: false
#| output: asis

summary(sdr4) %>% 
  as.data.frame() %>% 
  slice(-c(6:353)) %>%
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value4 = -fit4$objective
k = length(fit4$par)
aic_value4 = 2 * k - 2 * logLik_value4

# Calculate WAIC
log_lik4 = numeric(length(dat$y))

# Vector of predicted means
meanvec4 = c()
  for(i in 1:length(dat$y)){
    meanvec4[i] = (dat$eff[i]^pl4$alpha) * (dat$y[i] ^ pl4$beta)}
  
# Revert transformation of sigma(y)  
sdy = exp(pl4$logsd_C)

for(i in 1:length(dat$y)){
  log_lik4[i] = dnorm(dat$y[i], meanvec4[i], sdy, log=TRUE)
}

lppd4 = sum(log(mean(exp(log_lik4))))
p_waic4 = sum(var(log_lik4))
waic4 = -2 * (lppd4 - p_waic4)
```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)', ' $X_t = f(C_t)$'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4), 
                  WAIC = c(waic, waic2, waic3, waic4))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

For these models, we could try a different approach for the plots:

* Latent variable estimates (in purple) are extracted from the *sdr* object where we stored the report

* Predicted catch (in red) comes from the objective function that is specified to be reported during optimization (*REPORT (predictions)*). Because they do not incorporate dispersion they are not a good prediction for the catch, but are useful to see how realistic the optimization function was. If this series is unrealistic it is a symptom of an error in the objective function.

* Predicted catch (in green) are obtained from a single *rnorm* draw from the obtained parameters, as extracted from the *pl4* object, plus the estimate of the standard deviation of $C_t$ (these were the predictions reported in the previous models).

The lack of uncertainty estimate for $\sigma_C$ and the way the estimates for the latent variable follow the observation suggest overfitting. In hindsight, it should have been obvious from the beginning that the formulation for this model is so flexible that $X_t$ can neutralize the variations in $E_t$ and match the series perfectly, hence the null deviations. Without explicitly estimating the process for the latent variable to bound the process into its own domain, this model is pointless. 

```{r}
#| fig-width: 12
#| fig-height: 4

set.seed(1)

pred4 = c()
for(i in seq_along(meanvec4)){pred4[i] = rnorm(1, meanvec4[i],sdy)}

grid.arrange(
ggplot() +
    geom_point(aes(x = 1:length(dat$y),
                 y = dat$y)) +
  geom_line(aes(x = 1:length(dat$y),
                y = obj4$report()$predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$y),
                y = pred4),
            color = 'green') +
  geom_line(aes(x = 1:length(dat$y),
                y = summary(sdr4, "random")[,'Estimate']),
            color = 'purple') +
  geom_ribbon(aes(x = 1:length(dat$y),
                ymax = summary(sdr4, "random")[,'Estimate'] + 
                  2 * summary(sdr4, "random")[,'Std. Error'],
                ymin = summary(sdr4, "random")[,'Estimate'] - 
                  2 * summary(sdr4, "random")[,'Std. Error']),
            fill = 'purple', linetype = 2, alpha = 0.3) +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),

  ncol=1)
```

At this point I could have gone back and added the latent process, but I'd rather let this example stand as a reminder to future me about silly formulations and move on to the next model.

## Model 2.2: The Schaefer Surplus Production Model

Let $B_t$ be the stock biomass at time $t$; The most simplistic Schaefer formulation states that, if cephalopod stocks followed the usual equilibrium assumptions, then

$$
B_{t+1} = B_t +r \cdot B_t \left(1 - \frac{B_t}{K} \right) - C_t
$$

We will take a first pass at inference on $B_t$ by treating it as a latent variable and using $C_t$ as the observation sequence. The link between $B_t$ and $C_t$ is given by

$$
\frac{C_t}{E_t} = q \cdot B_t
$$

which means the likelihood function for $B_t$ is derived from its distribution being:

$$
B_t \sim N(q \cdot \frac{C_t}{E_t}, \sigma_{B_t})
$$

for this to work, we need to estimate $q$, $K$, $r$, $B_0$ as well as every point for $B_t$. Fun times! At this time, we will not attempt to directly model parameter uncertainty, because life is already too hard as it is.

```{r}
#| results: hide
#| warning: false
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort

# initialize parameter list with initial estimates
par = list()
par$Bt = as.vector(rep(10000000,length(dat$Ct)))# init empty vector
# par$meanBt = 100000

par$q = 0.5
par$r = 0.5
par$K = 4*max(dat$Ct)
# par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdBt = 1
par$logsdCt = 1
# par$logsdB0 = 1000
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  sdBt = exp(logsdBt)
  sdCt = exp(logsdCt)
  # sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  jnll = jnll - dexp(r,1, log = TRUE)
  jnll = jnll - dexp(q,0.1, log = TRUE)
  jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  
  # prior on B0
  jnll = jnll - dnorm(Bt[1], K/2, 1000, log = TRUE)
  
  ## From the observation equation
 
  ## From the process equation
  # Initial values
  for(i in 2:length(Ct)){
    predBt = Bt[i-1] + r * Bt[i-1]*(1-(Bt[i-1]/K)) - Ct[i-1]
    jnll = jnll - dnorm(Bt[i], predBt, sdBt, log=TRUE)}
  
 
  catch_predictions = c()
  for(i in 1:length(Ct)){
   Ct_hat = q*Bt[i]*Et[i]
   jnll = jnll -dnorm(Ct[i], Ct_hat, sdCt, log = TRUE)
   catch_predictions[i] = Ct_hat
  }
   
  REPORT(predBt)
  REPORT(catch_predictions)
  # REPORT(B0)
  REPORT(Bt)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj5 = MakeADFun(jnll, par, random = 'Bt')
fit5 = nlminb(obj5$par, obj5$fn, obj5$gr)

sdr5 = sdreport(obj5)
pl5 = as.list(sdr5,"Est")
plsd5 = as.list(sdr5,"Std")
```

\todo[inline]{NaN warnings were also produced here}

```{r}
#| output: asis
#| echo: false
#| warning: false
summary(sdr5) %>% 
  as.data.frame() %>% 
  slice(-c(6:357)) %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value5 = -fit5$objective
k = length(fit5$par)
aic_value5 = 2 * k - 2 * logLik_value5

# Calculate WAIC
log_lik5 = numeric(length(dat$Ct))

# Vector of predicted means
meanvec5 = c()
  for(i in 1:length(dat$Ct)){
    meanvec5[i] = fit5$par['q'] * 
      summary(sdr5, "random")[,'Estimate'][i] *
      dat$Et[i] }
  
# Revert transformation of sigma(y)  
sdy = exp(pl5$logsdCt)

for(i in 1:length(dat$Ct)){
  log_lik5[i] = dnorm(dat$Ct[i], meanvec5[i], sdy, log=TRUE)
}

lppd5 = sum(log(mean(exp(log_lik5))))
p_waic5 = sum(var(log_lik5))
waic5 = -2 * (lppd5 - p_waic5)

```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)',
                            ' $X_t = f(C_t)$', 'SPM'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4, aic_value5), 
                  WAIC = c(waic, waic2, waic3, waic4, waic5))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

The good news is, we were able to fit a model that yielded estimates and catch predictions that reside on this side of la la land, which is encouraging. However, the complete failure to estimate standard error for any parameter is catastrophic and realistically this model needs to go back to the drawing board. Maybe that is for the best, as if we were to believe the biomass estimates, catches would be dangerously close to depleting the stock!

```{r}
#| fig-width: 12
#| fig-height: 4
#| echo: false
#| warning: false

set.seed(1)

pred5 = c()
for(i in seq_along(meanvec5)){pred5[i] = rnorm(1, meanvec5[i],sdy)}

grid.arrange(
ggplot() +
  geom_point(aes(x = 1:length(dat$Ct),
                 y = dat$Ct)) + 
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj5$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = pred5),
            color = 'green') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = summary(sdr5, "random")[,'Estimate']),
            color = 'purple') +
  geom_ribbon(aes(x = 1:length(dat$Ct),
                ymax = summary(sdr5, "random")[,'Estimate'] + 
                  2 * summary(sdr5, "random")[,'Std. Error'],
                ymin = summary(sdr5, "random")[,'Estimate'] - 
                  2 * summary(sdr5, "random")[,'Std. Error']),
            fill = 'purple', linetype = 2, alpha = 0.3) +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),

  ncol=1)
```

For the sake of the exercise, we can examine the biological reference points that this model would lead to.

The maximum sustainable yield (*MSY*) in a Schaefer *SPM* can be quickly calculated as

```{r}
#| echo: false
#| results: hide

msy = pl5$r * pl5$K/ 4
pl5$K/2

```

$$
MSY = \frac{rk}{4} = \frac{0.55 \times 3049048}{4} = 422296.9
$$

Because we are assuming a symmetrical production curve, $B_{MSY} = K/2 =$ `r pl5$K/2`. From the earlier definitions, the surplus production each year is given by the overall biomass estimate at time $t$ minus the catch removal, or, in other words:

$$
P_t = rB_t \cdot (1-\frac{B}{K})
$$

We can generalize the production curve equilibrium for this model as:

```{r}
B = seq(0, pl5$K, by = pl5$K/10000) 
P = pl5$r * B * (1 - B / pl5$K)
  
ggplot() + 
  geom_point(aes(x = B,
                 y = P)) + 
  geom_vline(aes(xintercept = pl5$K/2),
             col = 'red',
             linetype = 2) +
  geom_hline(aes(yintercept = msy),
             col = 'red',
             linetype = 2) + 
  geom_point(aes(x = pred5,
                  y = summary(sdr5,
                              "random")[,'Estimate'])) +
  theme_bw() + 
  labs(title = 'Surplus Production Model (points = observations)')
```

So, if we were to trust this model (and the lack of standard error in the parameters should be a strong indicator not to!) then we would be looking at a stock whose biomass is on the very low end of the carrying capacity of its habitat and the fishing mortality being inflicted upon it has been consistently above the sustainability threshold.

## Model 2.3: A Surplus Production Model with the catch included as an autorregressive term

This variation was proposed by the instructors with the explicit goal of limiting the impact of the catch in the existing biomass. $C_t$ is shifted from an additive term to a multiplicative one, with the addition of a scaling coefficient $\alpha$, to keep its impact grounded between 0 and 1.

$$
B_{t+1} = B_t +r \cdot B_t \left(1 -\frac{B_t}{K} \right) \cdot \alpha C_t + \epsilon_t
$$

If we combine $\alpha$ and $r$ into a single constant $\alpha$, which could be seen as rescaled natural intrinsic growth, then we could arrange it as

$$
B_{t+1} = B_t \cdot (1 + \alpha ) \cdot \left(1 - \frac{B_t}{K} \right) \cdot C_t + \epsilon_t
$$

```{r}
#| results: hide
#| warning: false
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort

# initialize parameter list with initial estimates
par = list()
par$Bt = as.vector(rep(10000000,length(dat$Ct)))# init empty vector
# par$meanBt = 100000

par$q = 0.5
par$alpha = 0.5
par$K = 4*max(dat$Ct)
# par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdBt = 1
par$logsdCt = 1
# par$logsdB0 = 1000
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  sdBt = exp(logsdBt)
  sdCt = exp(logsdCt)
  # sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of alpha
  jnll = jnll - dexp(alpha,1, log = TRUE)
  jnll = jnll - dexp(q,0.1, log = TRUE)
  jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  
  # prior on B0
  jnll = jnll - dnorm(Bt[1], K/2, 1000, log = TRUE)
  
  ## From the observation equation
 
  ## From the process equation
  # Initial values
  for(i in 2:length(Ct)){
    predBt = Bt[i-1] + (1 + alpha) * (1-(Bt[i-1]/K)) * Ct[i-1]
    jnll = jnll - dnorm(Bt[i], predBt, sdBt, log=TRUE)}
  
 
  catch_predictions = c()
  for(i in 1:length(Ct)){
   Ct_hat = q*Bt[i]*Et[i]
   jnll = jnll -dnorm(Ct[i], Ct_hat, sdCt, log = TRUE)
   catch_predictions[i] = Ct_hat
  }
   
  REPORT(predBt)
  REPORT(catch_predictions)
  # REPORT(B0)
  REPORT(Bt)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj6 = MakeADFun(jnll, par, random = 'Bt')
fit6 = nlminb(obj6$par, obj6$fn, obj6$gr)

sdr6 = sdreport(obj6)
pl6 = as.list(sdr6,"Est")
plsd6 = as.list(sdr6,"Std")
```

\todo[inline]{NaN warnings were also produced here}

```{r}
#| output: asis
#| echo: false
#| warning: false
summary(sdr6) %>% 
  as.data.frame() %>% 
  slice(-c(6:357)) %>% 
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value6 = -fit6$objective
k = length(fit6$par)
aic_value6 = 2 * k - 2 * logLik_value6

# Calculate WAIC
log_lik6 = numeric(length(dat$Ct))

# Vector of predicted means
meanvec6 = c()
  for(i in 1:length(dat$Ct)){
    meanvec6[i] = fit6$par['q'] * 
      summary(sdr6, "random")[,'Estimate'][i] *
      dat$Et[i] }
  
# Revert transformation of sigma(y)  
sdy = exp(pl6$logsdCt)

for(i in 1:length(dat$Ct)){
  log_lik6[i] = dnorm(dat$Ct[i], meanvec6[i], sdy, log=TRUE)
}

lppd6 = sum(log(mean(exp(log_lik6))))
p_waic6 = sum(var(log_lik6))
waic6 = -2 * (lppd6 - p_waic6)

```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)',
                            ' $X_t = f(C_t)$', 'SPM', 'Custom SPM'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4, aic_value5, aic_value6), 
                  WAIC = c(waic, waic2, waic3, waic4, waic5, waic6))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

```{r}
#| fig-width: 12
#| fig-height: 4
#| echo: false
#| warning: false

set.seed(1)

pred6 = c()
for(i in seq_along(meanvec6)){pred6[i] = rnorm(1, meanvec6[i],sdy)}

grid.arrange(
ggplot() +
  geom_point(aes(x = 1:length(dat$Ct),
                 y = dat$Ct)) + 
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj6$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = pred6),
            color = 'green') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = summary(sdr6, "random")[,'Estimate']),
            color = 'purple') +
  geom_ribbon(aes(x = 1:length(dat$Ct),
                ymax = summary(sdr6, "random")[,'Estimate'] + 
                  2 * summary(sdr6, "random")[,'Std. Error'],
                ymin = summary(sdr6, "random")[,'Estimate'] - 
                  2 * summary(sdr6, "random")[,'Std. Error']),
            fill = 'purple', linetype = 2, alpha = 0.3) +
  theme_bw() +
   labs(title = 'data (points) and catch predictions (line)'),

  ncol=1)
```

The lack of standard errors, the failure of updating the initial values of most parameters, the complete flatlining of biomass estimates to values around 40 kg (!) indicate that either this formulation is a dead-end or some serious work is required into tweaking the starting conditions. Unfortunately, time is limited and there is yet a third approach to consider.

# Third act: The mask falls, and the AIC rises

So far we have considered basic autoregressive models, which are not very useful for inference, and surplus production models, which are a standard procedure for many data-limited fishing stocks but in the specific case of octopus have already been rejected as a viable option. A big part of my motivation in attending this workshop consisted in being able to explore myself a family of models that are currently seen as a workable solution for the octopus stock assessment problem: Generalized Depletion Models.

The basic intuition behind these models is that the initial biomass is steadily depleted by both natural and fishing mortality that occur at every step of a granular time series (weekly or monthly), and that depletion is countered, or reset, by big spikes of abundance driven by migration or recruitment events. This approach is used sucessfully to monitor squid fisheries in-season and has recently been expanded to acommodate octopus stocks. For the remainder of this report, we will consider the approach employed by Roa-Ureta *et-al.* that is published and distributed in the *CatDyn* package. 

## Formulation, step by step

Consider the condensed formulation of this model, where catch in numbers is modeled as a function of observed effort and several parameters.

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_1}
C_t = kE_t^\alpha N_t^\beta = m kE_t^\alpha f_t(M,N_0, C_{i<t}, R, S) \\
\textrm{with} \quad m = e^{-\frac{M}{2}}
    \end{aligned}
\end{equation}

The author typically considers several distributions for this process and tries them, along as several optimizers, in a systematic, grid-search-like approach.

In order to incorporate this model in RTBM, we will have to break down the abundance estimate ($N_t^\beta$) into chunks that are easier to process. First we present the complete formulation and then we'll break down the chunks of $f(t)$ *ie*, everything that is raised to $\beta$.

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
C_t = kE_t^\alpha  m \left( N_0 e^{-Mt} -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] + \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)} - \sum_{l=1}^{l=v} J_l S_l e^{-M(t-\upsilon_l} \right) ^ \beta
    \end{aligned}
\end{equation}

## Chunk 1: Exponential Population Growth

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch1}
N_0 e^{-Mt}
    \end{aligned}
\end{equation}

The first term consists simply of natural population decay driven by natural mortality $M$, as illustrated below. Unlike other models, birth-driven input (recruitment) is not incorporated at this stage in the model. The model is only concerned with individuals available to the fishery, so until recruits attain a size that makes them eligible they are not accounted for. Their entry into the fishery will be modelled as discrete spikes.

```{r}
#| echo: false
#| warning: false

N0 = 1000000
x = 1:348
M = 0.05

res_1 = c()
# chunk 1
for(tt in 1:length(x)){
ch1 = N0 * exp(-M*tt)

res_1[tt] = ch1
}

  ggplot() + 
  geom_line(aes(x = x,
                y = res_1)) + 
  theme_bw() + 
  labs(title = 'chunk 1')
```

## Chunk 2: Catch carry over

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_ch2}
 -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] 
    \end{aligned}
\end{equation}

This section incorporates the expected value for the catch in each year, with decay from natural mortality that intensifies the further you are from that year. It is explicitly recursive, unlike chunk 1, and therefore the code must account for that correctly. 

It is unclear if $C_i$ accounts for the *entire* estimate in previous iterations or simply previous observations. For this report, the second alternative was implemented in the objective function

## Chunk 3: Recruitment pulse input

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_2}
    \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)}
    \end{aligned}
\end{equation}

This chunk adds the recruitment pulses at timing $\tau_j$, of magnitude $R_j$, modulated by the time passed between $t$ and $\tau_j$. There is a quirk in this formulation regarding the use of an indicator vector $I_j$, which in all papers published by the author of CatDyn is said to be 0 when $\tau_j < t$ and 1 afterwards. This interpretation, combined with the double negative signals in the exponential component of the chunk, leads to a scenario where recruitment spikes that occur after the time step $t$ contribute to $C_t$, where as spikes before $t$ are nullified by the 0 in the indicator vector $I_j$. 

In this work, an alternative formulation that is sometimes used by the author where the indicator vector is ommited. This does not mitigate the exponent signal effect, however.

Finally, the timings for the perturbations in the vector $\tau_j$ must be manually inserted by the user after visual inspection of the data. The author has develloped an approach regarding catch spike statistics that will not be discussed here, so for the sake of this report $\tau_j$ will be inserted manually into the parameter list.

There is a similar chunk with an inverted signal that accounts for emigration pulses; in octopus fisheries this is thought to reflect the moments when females leave the fishery after spawning. Currently it is assumed, due to the fishing gears used in the portuguese fishery, that such an exit does not occur in this particular case.

## Parameters to be estimated

-   $\alpha$ is the abundance response

-   $\beta$ is the effort response

Both allow non-linearity for $E_t$ and $N_t$;

-   $k$ is a scaling factor

-   $M$ is the natural mortality (with $m = e^{-\frac{M}{2}}$)

-   $N_0$ is the initial abundance of the stock at $t_0$

-  $R_j$ are the recruitment pulse magnitudes

## Model 3.1: GDM, as seen on CatDyn

```{r}
#| results: hide
#| warning: false

dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb/df_effort$res)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort

indice_manual_2 = 
  list(
    12,12,12,12, 
    12,12,12,12,12,
    12,10,10,11,12,
    12,12,10,11,12,
    11,12,10,12,11,
    12,12,12,12,10)

for(i in 0:(length(indice_manual_2)-1)){
  indice_manual_2[[i+1]] = 12*i + indice_manual_2[[i+1]]
}

dat$u = unlist(indice_manual_2) # recruitment pulse timings

# initialize parameter list with initial estimates
par = list()
par$Rt = as.vector(rep(20000,length(dat$u)))# init empty vector
# par$meanBt = 100000

par$logalpha = log(0.85)
par$logbeta = log(0.85)
par$logK = log(0.0001)
par$logN0 = log(600000000)
par$logM = log(0.01)

# par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdCt = 1
# par$logsdB0 = 1000

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  alpha = exp(logalpha)
  beta = exp(logbeta)
  K = exp(logK)
  N0 = exp(logN0)
  M = exp(logM)
  sdCt = exp(logsdCt)
  # Rt = exp(logRt)
  
  
  # sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  # jnll = jnll - dexp(r,1, log = TRUE)
  # jnll = jnll - dexp(q,0.1, log = TRUE)
  # jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  
  # prior on N0
  # jnll = jnll - dnorm(Bt[1], K/2, 1000, log = TRUE)
  
  ## From the observation equation
 
  ## From the process equation
  # Initial values
 
  catch = c()
  for(mes in seq_along(dat$Ct)){
    
    m = exp(-M/2)
    
    # core
    
    core = K*(Et[mes]^alpha) * m 
    
    # M-driven decay
    
    part1 = N0*exp(-M*mes)
    
    # catch aggregation
     
    part2 = 0
    for(mes_ant in seq_len(length(dat$Ct)-1)){
      part2 = part2 + Ct[mes_ant]*exp(-M*(mes-mes_ant-1))}
    
    # Recruitment pulses
    
    part3 = 0
    for(j in seq_along(u[u<=mes])){
      part3 = part3 + Rt[j]*exp(-M*(mes-u[j]))}
    
    
    # Assemble
    pred = core * (part1 -m*part2 + part3)^beta
    catch[mes] = pred
    
    jnll = jnll - dnorm(Ct[mes], pred, sdCt, log = T)
  
  }
  REPORT(catch)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj7 = MakeADFun(jnll, par, random = 'Rt')
fit7 = nlminb(obj7$par, obj7$fn, obj7$gr)

sdr7 = sdreport(obj7)
pl7 = as.list(sdr7,"Est")
plsd7 = as.list(sdr7,"Std")
```

\todo[inline]{NaN warnings were also produced here}

```{r}
#| output: asis
#| echo: false
#| warning: false
summary(sdr7) %>% 
  as.data.frame() %>% 
  slice(-c(7:36)) %>%
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value7 = -fit7$objective
k = length(fit7$par)
aic_value7 = 2 * k - 2 * logLik_value7

# Calculate WAIC
log_lik7 = numeric(length(dat$Ct))

# Vector of predicted means
meanvec7 = c()
  for(mes in seq_along(dat$Ct)){
    
    m = exp(-exp(pl7$logM)/2)
    
    # core
    
    core = exp(pl7$logK)*(dat$Et[mes]^exp(pl7$logalpha)) * m 
    
    # M-driven decay
    
    part1 = exp(pl7$logN0)*exp(-exp(pl7$logM)*mes)
    
    # catch aggregation
     
    part2 = 0
    for(mes_ant in seq_len(length(dat$Ct)-1)){
      part2 = part2 + dat$Ct[mes_ant]*exp(-exp(pl7$logM)*(mes-mes_ant-1))}
    
    # Recruitment pulses
    
    part3 = 0
    for(j in seq_along(dat$u[dat$u<=mes])){
      part3 = part3 + 
        summary(sdr7, 'random')[,'Estimate'][j]*
        exp(-exp(pl7$logM)*(mes-dat$u[j]))}
    
    
    # Assemble
    pred = core * (part1 -m*part2 + part3)^exp(pl7$logbeta)
    meanvec7[mes] = pred}
  
# Revert transformation of sigma(y)  
sdy = exp(pl7$logsdCt)

for(i in 1:length(dat$Ct)){
  log_lik7[i] = dnorm(dat$Ct[i], meanvec7[i], sdy, log=TRUE)
}

lppd7 = sum(log(mean(exp(log_lik7))))
p_waic7 = sum(var(log_lik7))
waic7 = -2 * (lppd7 - p_waic7)

```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)',
                            ' $X_t = f(C_t)$', 'SPM', 'Custom SPM', 'GMD'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4, aic_value5, aic_value6, aic_value7), 
                  WAIC = c(waic, waic2, waic3, waic4, waic5, waic6, waic7))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

Convergence was problematic, but we did achieve plausible estimates. $M$ and $N0$ did not yield standard error estimates, and we have some negative values for $R_j$, which should not be possible. We have explicitly excluded the scenario of exit pulses, but if we attempt to bound $R_j$ to positive values the model convergence completely falls apart. Perhaps that premise should be reevaluated?

```{r}
#| echo: false

set.seed(1)
pred7 = rnorm(348,
              meanvec7,
              sdy)

ggplot() + 
  geom_point(aes(x = 1:348,
                y = dat$Ct)) +
   geom_line(aes(x = 1:348,
                y = pred7),
            color = 'green') + 
  geom_line(aes(x = 1:348,
                y = obj7$report()['catch'] %>% unlist()),
            color = 'red') + 
  theme_bw()
```

## Model 3.2: What if it rained on Model 1

Finally, as an exercise, we will try adding precipitation as a multiplicative term.

\begin{equation}
    \begin{aligned}
    \label{eq:catdyn_6}
C_t = kE_t^\alpha  m \left( N_0 e^{-Mt} -m \left[ \sum_{i=i}^{i=t-1} C_{i,i} e^{-M(t-i-1)} \right] + \sum_{j=1}^{j=u} I_j R_j e^{-M(t-\tau_j)} - \sum_{l=1}^{l=v} J_l S_l e^{-M(t-\upsilon_l} \right) ^ \beta \cdot P_t ^ \gamma
    \end{aligned}
\end{equation}


```{r}
#| results: hide
#| warning: false

dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb/df_effort$res)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort
dat$Pt = rep(precip, each = 12)

indice_manual_2 = 
  list(
    12,12,12,12, 
    12,12,12,12,12,
    12,10,10,11,12,
    12,12,10,11,12,
    11,12,10,12,11,
    12,12,12,12,10)

for(i in 0:(length(indice_manual_2)-1)){
  indice_manual_2[[i+1]] = 12*i + indice_manual_2[[i+1]]
}

dat$u = unlist(indice_manual_2) # recruitment pulse timings

# initialize parameter list with initial estimates
par = list()
par$Rt = as.vector(rep(200000,length(dat$u)))# init empty vector
# par$meanBt = 100000

par$logalpha = log(0.6)
par$logbeta = log(0.6)
par$loggamma = log(0.6)
par$logK = log(0.001)
par$logN0 = log(600000000)
par$logM = log(0.01)

# par$B0 = 0.5*par$K # % of stock depletion at time zero

par$logsdCt = 1
# par$logsdB0 = 1000

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  alpha = exp(logalpha)
  beta = exp(logbeta)
  gamma = exp(loggamma)
  K = exp(logK)
  N0 = exp(logN0)
  M = exp(logM)
  sdCt = exp(logsdCt)
  # Rt = exp(logRt)
  
  
  # sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  # jnll = jnll - dexp(r,1, log = TRUE)
  # jnll = jnll - dexp(q,0.1, log = TRUE)
  # jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  
  # prior on N0
  # jnll = jnll - dnorm(Bt[1], K/2, 1000, log = TRUE)
  
  ## From the observation equation
 
  ## From the process equation
  # Initial values
 
  catch = c()
  for(mes in seq_along(dat$Ct)){
    
    m = exp(-M/2)
    
    # core
    
    core = K*(Et[mes]^alpha) * m 
    
    # M-driven decay
    
    part1 = N0*exp(-M*mes)
    
    # catch aggregation
     
    part2 = 0
    for(mes_ant in seq_len(length(dat$Ct)-1)){
      part2 = part2 + Ct[mes_ant]*exp(-M*(mes-mes_ant-1))}
    
    # Recruitment pulses
    
    part3 = 0
    for(j in seq_along(u[u<=mes])){
      part3 = part3 + Rt[j]*exp(-M*(mes-u[j]))}
    
    
    # Assemble
    pred = core * (part1 -m*part2 + part3)^beta * Pt[mes]^gamma
    catch[mes] = pred
    
    jnll = jnll - dnorm(Ct[mes], pred, sdCt, log = T)
  
  }
  REPORT(catch)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj8 = MakeADFun(jnll, par, random = 'Rt')
fit8 = nlminb(obj8$par, obj8$fn, obj8$gr)

sdr8 = sdreport(obj8)
pl8 = as.list(sdr8,"Est")
plsd8 = as.list(sdr8,"Std")
```

\todo[inline]{NaN warnings were also produced here}

```{r}
#| output: asis
#| echo: false
#| warning: false
summary(sdr8) %>% 
  as.data.frame() %>% 
  slice(-c(7:36)) %>%
  xtable %>% 
  print(comment = F,
        sanitize.text.function = function(x) 
          {gsub("_", "\\\\_", x)})
```

```{r}
# Calculate AIC
logLik_value8 = -fit8$objective
k = length(fit8$par)
aic_value8 = 2 * k - 2 * logLik_value8

# Calculate WAIC
log_lik8 = numeric(length(dat$Ct))

# Vector of predicted means
meanvec8 = c()
  for(mes in seq_along(dat$Ct)){
    
    m = exp(-exp(pl8$logM)/2)
    
    # core
    
    core = exp(pl8$logK)*(dat$Et[mes]^exp(pl8$logalpha)) * m 
    
    # M-driven decay
    
    part1 = exp(pl8$logN0)*exp(-exp(pl8$logM)*mes)
    
    # catch aggregation
     
    part2 = 0
    for(mes_ant in seq_len(length(dat$Ct)-1)){
      part2 = part2 + dat$Ct[mes_ant]*exp(-exp(pl8$logM)*(mes-mes_ant-1))}
    
    # Recruitment pulses
    
    part3 = 0
    for(j in seq_along(dat$u[dat$u<=mes])){
      part3 = part3 + 
        summary(sdr8, 'random')[,'Estimate'][j]*
        exp(-exp(pl8$logM)*(mes-dat$u[j]))}
    
    
    # Assemble
    pred = core * (part1 -m*part2 + part3)^exp(pl8$logbeta) *
      dat$Pt[mes] ^ exp(pl8$loggamma)
    meanvec8[mes] = pred}
  
# Revert transformation of sigma(y)  
sdy = exp(pl8$logsdCt)

for(i in 1:length(dat$Ct)){
  log_lik8[i] = dnorm(dat$Ct[i], meanvec8[i], sdy, log=TRUE)
}

lppd8 = sum(log(mean(exp(log_lik8))))
p_waic8 = sum(var(log_lik8))
waic8 = -2 * (lppd8 - p_waic8)

```

```{r}
#| output: asis
#| echo: false

xtable(data.frame(Model = c('AR(1)', 'AR(1) + Pt', 'AR(1) + Pt (ii)',
                            ' $X_t = f(C_t)$', 'SPM', 'Custom SPM', 'GMD', 'GMD + Pt'),
                  AIC = c(aic_value, aic_value2, aic_value3, aic_value4, aic_value5, aic_value6, aic_value7, aic_value8), 
                  WAIC = c(waic, waic2, waic3, waic4, waic5, waic6, waic7, waic8))) %>% 
  print(comment = F,
        sanitize.text.function = function(x)
          {gsub("_", "\\\\_", x)})
```

Convergence times became larger by orders of magnitude with the addition of the new term. It is obvious that this model is extremely sensitive to the starting values of $\alpha$, $\beta$ and $\gamma$. The lack of standard error metrics in some parameters is still a problem.

Most initial attempts yielded very low estimates for $\sigma_C$, in the orders of the tens of kg, which means the catch process was effectively deterministic. It took more than 20 tries to get a significantly different prediction (green line below).

It is very hard to call this a success, but it a tentative approach in incorporating aditional variables in the original formulation.

```{r}
#| echo: false
#| warning: false

set.seed(1)
pred8 = rnorm(348,
              meanvec8,
              sdy)

ggplot() + 
  geom_point(aes(x = 1:348,
                y = dat$Ct)) +
   geom_line(aes(x = 1:348,
                y = pred8),
            color = 'green') + 
  geom_line(aes(x = 1:348,
                y = obj8$report()['catch'] %>% unlist()),
            color = 'red') + 
  theme_bw()
```

# Conclusion and Final Remarks

As stated initially, achieving brilliant insights into stock assessment for the portuguese octopus was not the main goal for attending this workshop. And even though I was very diligent in avoiding good decisions for the choice of models, some interesting observations were made along the way:

* The surplus approach led me to examine closely the catch-effort dynamic and accept that, without a better abundance index, the only conclusion possible is that there is no contrast in the data, ie the stock is always at a very low level regarding it carrying capacity. But if the stock is so close to scarcity and there is no reflection of that in the capture per effort observations, then truly this model is inadequate to describe the fishery.

* The fact that both generalized depletion models would only converge if recruitment pulses were allowed to be negative sugest that the premise where there are no significant exit events from the fishery should be reassessed. This is a key component of the approach and should be also tried in the original methodology in *CatDyn*, which will be the next step.

Overall, I found *RTMB* a brilliant tool not only for its flexibility and ability to succeed in modelling, but also in the way it fails so transparently. I think the workshop did a terrific job of covering different applications for this approach and I look forward to actually build a non-silly model for this use case, iterate upon it and explore it in depth.

Thank you.

# Addendum: Some notes on RTMB running procedures for future reference:

#### Parameters can be initialized without explicit assumptions on their prior distribution. Doing so will have the prior being assumed to be uniform:

```{r}
#| eval: false

par$p = 0
```

#### In order to explicitly set a prior distribution, we need to incorporate it in the likelihood estimation. Here I'm setting the prior distribution for $p$:

$$
p \sim N(\mu_p, \sigma_p)
$$

these priors are incorporated in the likelihood constructor function as **constants**, not *parameters to be estimated*:

```{r}
#| eval: false

par$p = 0

# these are priors, not parameters, so we write them like this:
prior_mean_p = 0
prior_sigma_p = 10

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  
  ## Prior distributions
  jnll = jnll - dnorm(p, prior_mean_p, prior_sd_p, log = TRUE)
  (...)
  }
```

#### The optimization in TMB is performed over unconstrained real numbers. It is a good practice to ensure that parameter estimates are kept within their realistic bounds by applying a transformation:

```         
Input the transformed value in the **parameter** section

Reverse the transformation inside the likelihood constructor function

Optimize the transformed, unconstrained parameter

Reverse the transformation
```

Here are some useful examples:

##### Log transformation: $p \in [0, + \infty]$

```{r}
#| eval: false

par$log_p = log(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(log_p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

##### Logit transformation: $p \in [0,1]$

```{r}
#| eval: false

par$logit_p = log(p/(1-p))

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = exp(logit_p)/(1+exp(logit_p))
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

##### Tanh transformation: $p \in [-1,1]$

```{r}
#| eval: false

par$atanh_p = atanh(p) 

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  (...)
  
  # assemble model    
  jnll = 0 # init jnll
  # reverse transformation
  p = tanh(p)
  ## Prior distributions
  jnll = jnll - dnorm(p, 0, 1000, log = TRUE)
  (...)
  }
```

#### One can add explicit boundaries for the parameters by adding them as named vectors to the parameter list:

```{r}
# Bounds for parameters in TMB
par$lower = list(r = 0, q = 0.01, K = 100, B0 = 0, logsdBt = -10, logsdCt = -10, logsdB0 = 0)
par$upper = list(r = 5, q = 10, K = 100000, B0 = par$K, logsdBt = 10, logsdCt = 10, logsdB0 = 10000)

```

#### A procedure to test the robustness of the model (as sugested by Anders Nielsen)

```{r}
#| results: hide
subject = obj
jit = replicate(100, 
                nlminb(subject$par+rnorm(length(subject$par),
                                         sd=.25),subject$fn,subject$gr)$par)
```

```{r}
jitplots = list()
for(i in 1: dim(jit)[1]){
  
  cor = colorRampPalette(wes_palette('Zissou1'))(dim(jit)[1])[i]
  # run in case of latent variables to avoid exploding the grOb
  if(dim(jit)[1] > 348 & i <= 348){next}
  jitplots[[length(jitplots)+1]] = 
    ggplot(data = data.frame(y = jit[i,])) + 
    geom_boxplot(aes(y=y), color = cor) + 
    labs(title = row.names(jit)[i],
         y = '') + 
    theme_bw()
   
}

grid.arrange(grobs = jitplots, nrow = 1)

apply(jit-fit$par,1,range)
```
