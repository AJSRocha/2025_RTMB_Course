---
title: "Custom SEM"
format: pdf
editor: visual
---

# Setup

```{r}
#| message: false
#| warning: false

# install.packages('TMB', type = 'source', lib = .libPaths()[2]) 
library('RTMB', lib.loc = .libPaths()[2])
library('tidyverse')
library(CatDyn)
library(tmbstan)
library(shinystan)

load("data/df_effort_m_mbw_otb.Rdata")
# sample data for testing purposes
iotc = read.csv('data/IOTC-DATASETS-2025-03-13-CELongline.csv')

iotc =
iotc %>% filter(Fleet == 'JPN    ') %>%
  filter(!is.na(YFT.NO)) %>% 
  group_by(Year, MonthStart) %>% 
  summarise(Effort = sum(Effort, na.rm =T),
            Catch = sum(YFT.NO, na.rm = T ) *0.01955034)

precip = c(-16.67,
           115.33, 245.93, 193.93, -229.47,-76.47,
           250.33, 177.73, 115.63, 90.13, -299.57,
           -338.37,97.18, -317.67, -198.74,-14.07,
           221.67, -90.97, -205.97, 98.03, 256.73,
           -241.87, 150.13, -300.17, 98.43, -85.87,
           -94.67,-156.87,-43.87,-106.37) + 
  735.8 + 105.7
anos = 1994:2023

mod_aux = lm(df_effort$catch ~ df_effort$effort)

df_effort = 
  df_effort %>% 
  mutate(catch = case_when(year_sale == 2005 & month_sale == '09' ~ 
                              effort * mod_aux$coefficients[2] +  mod_aux$coefficients[1],
         T ~ catch),
         catch_otb = case_when(year_sale == 2005 & month_sale == '09' ~ 
                                  effort_otb * mod_aux$coefficients[2] + mod_aux$coefficients[1],
                            T ~ catch_otb)) %>% 
  mutate(catch_otb = case_when(catch_otb == 0 ~ mean(catch_otb), 
                               T ~ catch_otb),
         effort_otb = case_when(effort_otb == 4 ~ mean(effort_otb),
                                T ~ effort_otb))

``` 

# Data

## Catch per Unit of Effort $CPUE_t$

Catch is obtained via reported landings. It is converted to number of individuals via a mean body weight model (not described here). Catches in MAGD are typically modeled in numbers.

Since we are dealing with a polyvalent fishery that employs a wide range of fishing gears and operates in vessels that are small enough to be exempt from keeping logbooks, effort data can be estimated at best by attempting to count fishing days in each month.

```{r}
df_effort %>% 
  ggplot() + 
  geom_line(aes(x = 1:348,
                y = catch_otb/effort_otb)) + 
  theme_bw() + 
  labs(title = 'nominal CPUE (kg/day)')
```

## Yearly precipitation data for the region, to be incorporated at a later date

```{r}
  ggplot() + 
  geom_line(aes(x = anos,
                y = precip)) + 
  theme_bw() + 
  labs(title = 'Yearly precipitation')
```

# Defining the model

We have a biomass time series that is unobservable and therefore, latent. We can use our measurements for Capture and Effort at month $t$ ($C_t$ and $E_t$). Our challenge will come from incorporating the precipitation from the previous year, $P_{t-1}$   

## Basic model

In this model we estimate $phi$ and $C_{0}$ (which the catch at $t = 0$ and not actually part of the catch series, hence why the need for estimation). Parameters are modeled without explicit prior distributions, meaning a flat uniform distribution will be assumed by **RTMB**. 

$$
C_{t+1} = \phi \cdot C_t  + \epsilon_t
$$

```{r}
#| eval: true
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$y = as.vector(df_effort$catch_otb)  # catches

# initialize parameter list with initial estimates
par = list()
par$logSdY = 1 # We must ensure positive numbers, so we work with logs
par$phi = 1
par$y0 = mean(dat$y)

# if we treat phi and alpha as priors, then we must specify the distribution
# therefore we would need to add dispersion paramaters
# If we fail to do so, a flat uniform distribution is defaulted to 
# par$logsdphi = 0
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  sdY = exp(logSdY)
  # sdalpha = exp(logsdalpha) 
  # sdphi = exp(logsdphi) # will be used by dnorm below
  
  # assemble model    
  jnll = 0 # init jnll
  ## from alpha
  # jnll = jnll - dnorm(alpha, 1, log = TRUE)
  ## from phi
  # jnll = jnll - dnorm(phi, 1, log = TRUE)
  ## from y0
  # jnll = jnll - dnorm(y0, 1, log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(y)){
    if(i == 1) {predY = phi*y0}
    else{predY = phi*y[i-1]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")
```

## Second model: Added precipitation

In this version, we added rainfall $P_t$ from the previous year to the model. The series is actually lagged, so any given $P_{1995}$ will actually refer to the year of 1994, *eg*. This was done for ease of integration. 

$$
C_{t+1} = \phi \cdot C_t + \alpha\cdot P_t + \epsilon_t
$$

```{r}
#| eval: true
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

# initialize parameter list with initial estimates
par = list()
par$logSdY = 1 # We must ensure positive numbers, so we work with logs
par$phi = 1
par$alpha = 1
par$y0 = mean(dat$y)

# if we treat phi and alpha as priors, then we must specify the distribution
# therefore we would need to add dispersion parameters
# If we fail to do so, a flat uniform distribution is defaulted to 
# par$logsdphi = 0
# par$logsdalpha = 0
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  sdY = exp(logSdY)
  # sdalpha = exp(logsdalpha) 
  # sdphi = exp(logsdphi) # will be used by dnorm below
  
  # assemble model    
  jnll = 0 # init jnll
  ## from alpha
  # jnll = jnll - dnorm(alpha, 1, log = TRUE)
  ## from phi
  # jnll = jnll - dnorm(phi, 1, log = TRUE)
  ## from y0
  # jnll = jnll - dnorm(y0, 1, log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(x)){
    if(i == 1) {predY = phi*y0 + alpha*x[i]}
    else{predY = phi*y[i-1]+ alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj2 = MakeADFun(jnll, par)
fit2 = nlminb(obj2$par, obj2$fn, obj2$gr)

sdr2 = sdreport(obj2)
pl2 = as.list(sdr2,"Est")
plsd2 = as.list(sdr2,"Std")
```

## Third model: Now with hard priors

We account for uncertainty in $\phi$, $\alpha$ and $\sigma_{t}$, so we include distribution parameters for these quantities to be estimated.

```{r}
#| eval: true
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$y = as.vector(df_effort$catch_otb)  # catches
dat$x = as.vector(rep(precip[1:29], each = 12)) # rainfall

dat$y

# initialize parameter list with initial estimates
par = list()
par$logSdY = 1 # We must ensure positive numbers, so we work with logs
par$phi = 1
par$alpha = 1
par$y0 = mean(dat$y)

# if we treat phi and alpha as priors, then we must specify the distribution
# therefore we would need to add dispersion parameters
# If we fail to do so, a flat uniform distribution is defaulted to 
par$logsdphi = 0
par$logsdalpha = 0
par$logsdy0 = 0
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  y = OBS(y)
  sdY = exp(logSdY)
  sdalpha = exp(logsdalpha)
  sdphi = exp(logsdphi) # will be used by dnorm below
  sdy0 = exp(logsdy0)
  
  # assemble model    
  jnll = 0 # init jnll
  ## from alpha
  jnll = jnll - dnorm(alpha, sdalpha, log = TRUE)
  ## from phi
  jnll = jnll - dnorm(phi, sdphi, log = TRUE)
  ## from y0
  jnll = jnll - dnorm(y0, sdy0, log = TRUE)
  
  ## From the observation process
  predictions = c()
  for(i in 1:length(x)){
    if(i == 1) {predY = phi*y0 + alpha*x[i]}
    else{predY = phi*y[i-1]+ alpha*x[i]}
    jnll = jnll - dnorm(y[i], predY, sdY, log=TRUE)
    predictions[i] = predY
  }
  REPORT(predictions)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj3 = MakeADFun(jnll, par)
fit3 = nlminb(obj3$par, obj3$fn, obj3$gr)

sdr3 = sdreport(obj3)
pl3 = as.list(sdr3,"Est")
plsd3 = as.list(sdr3,"Std")

sdr3
```


```{r}
#| eval: false
#| # library(tmbstan)
fit2 = tmbstan(obj,chains=1,iter=1000)
# library(shinystan)
launch_shinystan(fit2)
```

### Prediction plots (generated from the deterministic part of the jnll function)

```{r}
  df_effort %>% 
  ggplot() + 
  geom_line(aes(y = dat$y,
                x = 1:348)) +
    geom_line(aes(x = 1:348,
                y = obj2$report()$predictions),
            col = 'red') +
  geom_line(aes(x = 1:348,
                y = obj$report()$predictions),
            col = 'green') +
  theme_bw() + 
  labs(title = 'With (green) and without (red) precipitation')
```

### Prediction plots from generated data via the parameters

```{r}
yhat = c()
for(i in 1:length(dat$y)){
  if(i == 1){yhat[i] = par$y0}
  else{yhat[i] = yhat[i-1]*sdr2$par.fixed[['phi']] +
    sdr2$par.fixed[['alpha']]*dat$x[i] +
    rnorm(1,0,exp(sdr2$par.fixed[['logSdY']]))}
}

yhat_no_p = c()
for(i in 1:length(dat$y)){
  if(i == 1){yhat_no_p[i] = par$y0}
  else{yhat_no_p[i] = yhat_no_p[i-1]*sdr2$par.fixed[['phi']]+
    rnorm(1,0,exp(sdr2$par.fixed[['logSdY']]))}
}

df_effort %>% 
  ggplot() + 
  geom_line(aes(y = dat$y,
                x = 1:348)) +
  geom_line(aes(y = yhat,
                x = 1:348),
            col = 'green') +
  geom_line(aes(x = 1:348,
                y = yhat_no_p),
            col = 'red') +
  theme_bw() + 
  labs(title = 'Simulated data with (green) and without (red) precipitation')
```

## First attempt at a state-approach

Let $B_t$ be the stock biomass at time $t$; The most simplistic Schaefer formulation states that, if cephalopod stocks followed the usual equilibrium assumptions, then

$$
B_{t+1} = B_t +r \cdot B_t \left(1 - \frac{B_t}{K} \right) - C_t
$$

We will take a first pass at inference on $B_t$ by treating it as a latent variable and using $C_t$ as the observation sequence. The link between $B_t$ and $C_t$ is given by

$$
\frac{C_t}{E_t} = q \cdot B_t
$$

which means the likelihood function for $B_t$ is derived from its distribution being:

$$
B_t \sim N(q \cdot \frac{C_t}{E_t}, \sigma_{B_t})
$$

for this to work, we need to estimate $q$, $K$, $r$, $B_0$ as well as every point for $B_t$. Fun times! At this time, we will not attempt to directly model parameter uncertainty nor the process error, because life is already too hard as it is.


```{r}
#| eval: true
dat = list()

# DO NOT USE DATA FRAMES!
# P_{t-1} = lagP

dat$Ct = as.vector(df_effort$catch_otb)  # catches
dat$Et = as.vector(df_effort$effort_otb) # effort
# dat$Ct = as.vector(iotc$Catch)
# dat$Et = as.vector(iotc$Effort)

# initialize parameter list with initial estimates
par = list()
par$Bt = as.vector(rep(10,length(dat$Ct)))# init empty vector
par$meanBt = 100000

par$q = 0.2
par$r = 0.5
par$K = 4*max(dat$Ct)
par$B0 = 0.9*par$K # % of stock depletion at time zero

par$logsdBt = 1
par$logsdCt = 1
par$logsdB0 = 1000
# First pass: Linear model, lol

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data
  
  # gather and cleanup pars
  Ct = OBS(Ct)
  sdBt = exp(logsdBt)
  sdCt = exp(logsdCt)
  sdB0 = exp(logsdB0)
  
  # assemble model    
  jnll = 0 # init jnll
  
 
  ## Prior distributions
  ## The 1 in the mean fixes the prior distribution of r
  jnll = jnll - dexp(r,1, log = TRUE)
  jnll = jnll - dexp(q,0.1, log = TRUE)
  jnll = jnll - dnorm(K, 1000000, 1000, log = TRUE)
  ## From the observation equation
 
  
  ## From the process equation
  Bt[1] = B0
  for(i in 2:length(Ct)){
    Bt[i] = Bt[i-1] + r * Bt[i-1]*(1-(Bt[i-1]/K)) - Ct[i-1]
    predBt = Bt[i]
    jnll = jnll - dnorm(predBt, predBt, sdBt, log=TRUE)
    
  }
  
  catch_predictions = c()
  for(i in 1:length(Ct)){
   Ct_hat = q*Bt[i]*Et[i]
   jnll = jnll -dnorm(Ct[i], Ct_hat, sdCt, log = TRUE)
   catch_predictions[i] = Ct_hat
  }
   

  REPORT(catch_predictions)
  REPORT(B0)
  REPORT(Bt)
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj4 = MakeADFun(jnll, par)
fit4 = nlminb(obj4$par, obj4$fn, obj4$gr)

sdr4 = sdreport(obj4)
pl4 = as.list(sdr4,"Est")
plsd4 = as.list(sdr4,"Std")

# sdr4
obj4$report()$Bt
plot(obj4$report()$Bt)
```

### Plotting Biomass estimates

```{r}
ggplot() +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$catch_predictions),
            color = 'red') +
  geom_point(aes(x = 1:length(dat$Ct),
                y = dat$Ct)) +
  theme_bw()



temp = data.frame(x = obj4$report()$Bt) %>% 
  mutate(y = sdr4$par.fixed[['r']]*x *
           (1-(x/sdr4$par.fixed[['K']])))

ggplot() +
  geom_point(aes(y  = temp$y,
                x = temp$x),
            color = 'red') +

  theme_bw()


ggplot() +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$catch_predictions),
            color = 'red') +
  geom_line(aes(x = 1:length(dat$Ct),
                y = obj4$report()$Bt),
            color = 'purple') +  
  geom_point(aes(x = 1:length(dat$Ct),
                y = dat$Ct)) +
  theme_bw()
```




```{r}
df_effort %>% 
  ggplot() + 
  geom_point(aes(y = catch_otb/effort_otb,
                x = effort_otb)) + 
  theme_bw() + 
  labs(title = 'Catch ~ Effort')
```
```{r}
modelo = lm(df_effort$catch_otb/df_effort$effort_otb ~ df_effort$effort_otb)

a = modelo$coefficients[["df_effort$effort_otb"]]
b = modelo$coefficients[['(Intercept)']]

df_effort %>% 
ggplot() + 
  geom_point(aes(x = effort_otb,
                 y = catch_otb/effort_otb)) + 
  geom_abline(slope = a, intercept = b, color = 'red') + 
  theme_bw()

```

```{r}
novo = df_effort %>% 
  mutate(
  cpue_hat = predict(modelo, newdata = df_effort),
                  Y = cpue_hat * df_effort$effort_otb)

novo %>% 
  ggplot() +
  geom_point(aes(x = effort_otb,
                 y = catch_otb), col = 'red') + 
  geom_point(aes(x = effort_otb,
                 y = Y))

novo
```

# Report starts here

# Foreword

# Data description and context

## Catch

For the purposes of this course, 2 outliers were forcefully removed from the dataset. One pertained to an imposed closure on the fishery, while the other was an abnormally low activity month, with 4 fishing days in the entire fleet and 0 octopus landed. In a serious report on this data these gaps would have to be explained and accounted for, but for simplicity sake here they were replaced by the linear prediction of the series in the first outlier (catch-only) and the mean of the series (abnormal catch and effort).

## Effort

## Precipitation

## mean body weight

# First act: I flunked my time series course twice

Mention on how some correlation was observed

## Model 1: Baseline

## Model 2: With Precipitation

# Second act: I did come to a SEM course, after all

## Model 1: True catch as a latent variable

Following up on the previous idea, the instructors proposed the notion of the true catch being a latent variable whose process equation could be described by an **AR(p)** time series, and the observed catch $C_t$ as a result of the failure to realize or observe the maximum potential catch.

$$
\begin{cases}
  Process: X_t = \phi X_{t-1} + \epsilon_t \\
  Observation: C_t = \alpha X_t + \dots
\end{cases}
$$

## Model 2: The Schaefer Surplus Production Model

## Model 3: A Surplus Production Model with the catch included as an autorregressive term

This variation was proposed by the instructors with the explicit goal of limiting the impact of the catch in the existing biomass. $C_t$ is
shifted from an additive term to a multiplicative one, with the addition of a scaling coefficient $\alpha$, to keep its impact grounded between 0 and 1.


$$
B_{t+1} = B_t +r \cdot B_t \left(1 -\frac{B_t}{K} \right) \cdot \alpha C_t + \epsilon_t
$$

If we combine $\alpha$ and $r$ into a single constant $\alpha$, which could be seen as rescaled natural intrinsic growth, then we could arrange it as

$$
B_{t+1} = B_t \cdot (1 + \alpha ) \cdot \left(1 - \frac{B_t}{K} \right) \cdot C_t + \epsilon_t
$$

# Third act: The mask falls, and the AIC rises

## Model 1: GDM, as seen on CatDyn

## Model 2: What if it rained on Model 1

# Conclusion and Final Remarks.