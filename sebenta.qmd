---
title: "RTMB - 1"
format: pdf
editor: visual
---

# Setup

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
#| message: false
#| warning: false

# install.packages('TMB', type = 'source', lib = .libPaths()[2]) 
library('RTMB', lib.loc = .libPaths()[2])
library('tidyverse')
library(tmbstan)
library(shinystan)
library(loo)

library(leaflet)
library(viridis)
library(fmesher)

library(gridExtra)
library(readxl)

```

# Day 1

* Parametric vs non-parametric functions
* Parametric functions rely on probability distributions with their respective **likelihood functions**.

Sometimes the mean is not a parameter in the distribution. It may be advisable, for some data, to include the mean in the probability density function (*pdf*) so we are able to model it.

## Edge cases: Zero-inflated data

## Edge cases: Censored data

Data that, for some reason such as detection limits or time constraints, is understood to only reflect a subset or a small window of the complete range of the distribution.

Another good example would be truncated data, ie a machine that records 10 for each reading that goes over 10, ie *flatlining*.

## The difference between *pdf* and likelihood functions:

Likelihood is a function of the *pdf*:

$$
L(Y|\theta) = \prod_{i=1}^n PDF(y_i,\theta)
$$


## Loglikelihood functions in **R**:

Given a vector *Y*, can we assess the loglikelihood of their distribution being normal with $\mu = 4$ and $\sigma =1$ ?

```{r}
# data points
Y = c(5,4,3,6)

# loglikelihood for each point
res = dnorm(c(5,4,3,6), 4, 1, log = T)

# loglikelihood for this set of parameters and this data set
sum(res) 
```
## Metropolis - Hastings algorithm for MCMC

* Consider the parameter $U$
* Take a guess for the value for $U$: $U^0 = u$
* Begin iteration $t$
* Take a candidate distribution with density $q(u)$
* Draw $v$ from $q(u)$
* Compute 

$$
r = \frac{P(v|Y)}{P(u|Y)}
$$

* Draw a random number $z$ such as $z \in [0,1]$
* Update $U^t$: if $r > z, U^t = v$
* Else, if $r < z, U^t = u$
* Iterate 2-5 times

## Example in RTMB

First, by hand:

```{r}
trueA = 5
trueB = 0
trueSd = 100
sampleSize = 1000

# create independent x-values 
x = (-(sampleSize-1)/2):((sampleSize-1)/2)

# create dependent values according to ax + b + N(0,sd)
y =  trueA * x + trueB + rnorm(n=sampleSize,mean=0,sd=trueSd)
plot(x,y, main="Test Data")


likelihood = function(param){
  a = param[1]
  b = param[2]
  sd = param[3]
  
  pred = a*x + b
  singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = T)
  sumll = sum(singlelikelihoods)
  return(sumll)    
}

# Example: plot the likelihood profile of the slope a
slopevalues = function(x){
  return(likelihood(c(x, trueB, trueSd)))}

slopelikelihoods = lapply(seq(3, 7, by=.05), slopevalues )
plot (seq(3, 7, by=.05),
      slopelikelihoods ,
      type="l",
      xlab = "values of slope parameter a",
      ylab = "Log likelihood")

# Prior distribution
prior = function(param){
  a = param[1]
  b = param[2]
  sd = param[3]
  aprior = dunif(a, min=0, max=10, log = T)
  bprior = dnorm(b, sd = 5, log = T)
  sdprior = dunif(sd, min=0, max=300, log = T)
  return(aprior+bprior+sdprior)
}

posterior = function(param){
  return (likelihood(param) + prior(param))
}

######## Metropolis algorithm ################
proposalfunction = function(param){
  return(rnorm(3,mean = param, sd= c(0.1,0.5,0.3)))
}

run_metropolis_MCMC = function(startvalue, iterations){
  chain = array(dim = c(iterations+1,3))
  chain[1,] = startvalue
  for (i in 1:iterations){
    proposal = proposalfunction(chain[i,])
    
    probab = exp(posterior(proposal) - posterior(chain[i,]))
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

startvalue = c(4,0,10)
chain = run_metropolis_MCMC(startvalue, 100000)
burnIn = 5000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))

### Summary: #######################
par(mfrow = c(2,3))
hist(chain[-(1:burnIn),1],nclass=30, ,
     main="Posterior of a", xlab="True value = red line" )
abline(v = mean(chain[-(1:burnIn),1]))
abline(v = trueA, col="red" )
hist(chain[-(1:burnIn),2],nclass=30, 
     main="Posterior of b", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),2]))
abline(v = trueB, col="red" )
hist(chain[-(1:burnIn),3],nclass=30, 
     main="Posterior of sd", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),3]) )
abline(v = trueSd, col="red" )
plot(chain[-(1:burnIn),1], type = "l", 
     xlab="True value = red line" , 
     main = "Chain values of a", )
abline(h = trueA, col="red" )
plot(chain[-(1:burnIn),2], type = "l", 
     xlab="True value = red line" , 
     main = "Chain values of b", )
abline(h = trueB, col="red" )
plot(chain[-(1:burnIn),3], type = "l", 
     xlab="True value = red line" , 
     main = "Chain values of sd", )
abline(h = trueSd, col="red" )
# for comparison:
summary(lm(y~x))


```

Now with RTMB:

```{r}
# library(RTMB)
dat = list()

# getting the data from the previous example.
# DO NOT USE DATA FRAMES!
dat$y1 = as.vector(y)
dat$x1 = as.vector(x)

# initialize parameter list with initial estimates
par = list()
par$logSdy = 5 # We must ensure positive numbers, so we work with logs
par$b = 0
par$a = 1

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data 
  sdy = exp(logSdy) # will be used by dnorm below
  jnll = 0 # init jnll
  for(i in 1:length(y1)){
      predy = a*x1[i]+b
      jnll = jnll - dnorm(y1[i], predy, sdy, log=TRUE)
  }
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")

# library(tmbstan)
fit2 = tmbstan(obj,chains=1,iter=1000)

```


```{r}
#| eval: false
# library(shinystan)
launch_shinystan(fit2)
```

## Other examples

```{r}
# library(RTMB)
trueA = 10
trueB = 2
trueSd = 1009
sampleSize = 10000

# create independent x-values 
x = (-(sampleSize-1)/2):((sampleSize-1)/2)

# create dependent values according to ax + b + N(0,sd)
y =  trueA * x + trueB + rnorm(n=sampleSize,mean=0,sd=trueSd)
plot(x,y, main="Test Data")


dat = list()

# getting the data from the previous example.
# DO NOT USE DATA FRAMES!
dat$y1 = as.vector(y)
dat$x1 = as.vector(x)

# initialize parameter list with initial estimates
par = list()
par$logSdy = 5 # We must ensure positive numbers, so we work with logs
par$b = 1
par$a = 1

# initialize joint negative loglikelihood function
jnll = function(par){
  getAll(par, dat) # invoke all the parameters and data 
  sdy = exp(logSdy) # will be used by dnorm below
  jnll = 0 # init jnll
  for(i in 1:length(y1)){
      predy = a*x1[i]+b
      jnll = jnll - dnorm(y1[i], predy, sdy, log=TRUE)
  }
  jnll
}

# quick test: do we get a number? This number should be a likelihood.
jnll(par)

obj = MakeADFun(jnll, par)
fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")

# library(tmbstan)
fit2 = tmbstan(obj,chains=1,iter=1000)

```

# Day 2

## Multiple Regression/ Validation

```{r}
peru = read.csv('data/Peru.csv')

peru$WEIGHT = as.numeric(peru$WEIGHT)
peru$CHIN = as.numeric(peru$CHIN)
summary(peru)

dat = list()
dat$Y = as.numeric(peru$SYSTOL)
dat$X = as.matrix(peru %>% select(AGE,YEARS,WEIGHT,HEIGHT,CHIN,FOREARM,CALF,PULSE))
#dat$X <- as.matrix(data %>% select(WEIGHT,CHIN))

par = list()
par$logSdObs = 0 # SD from observation process
par$int = 0 # intercept
par$beta = as.vector(rep(0.1, ncol(dat$X)))
#par$loglambda <-as.vector(rep(0.1, ncol(dat$X)))

jnll = function(par){
  getAll(par, dat)
  Y = OBS(Y) # defines Y as observed value  
  sdObs = exp(logSdObs) # reverses log for modelling
#  lambda <- exp(loglambda)
  jnll = 0
#  for(i in 1:length(beta)){           # horseshoe prior
#    jnll <- jnll - dnorm(beta[i], 0, 1.9*lambda[i], log=TRUE) 
#    jnll <- jnll - log(2/(pi+pi*lambda[i]^2))
#  }
  meanvec = int + as.vector(beta %*% t(X))
  for(i in 1:length(Y)){
    jnll = jnll - dnorm(Y[i], meanvec[i], sdObs, log=TRUE) 
  } 
  ADREPORT(sdObs)
  jnll
}

jnll(par)

obj = MakeADFun(jnll, par)

fit = nlminb(obj$par, obj$fn, obj$gr)

sdr = sdreport(obj)
pl = as.list(sdr,"Est")
plsd = as.list(sdr,"Std")
pladd = as.list(sdr,"Est",report=TRUE)
plsdadd = as.list(sdr,"Std",report=TRUE)

# One-Step-Ahead Prediction
osa = oneStepPredict(obj)
qqnorm(osa$res); abline(0,1)

# Calculate AIC
logLik_value = -fit$objective
k = length(fit$par)
aic_value = 2 * k - 2 * logLik_value
print(aic_value)

# Calculate WAIC
log_lik = numeric(length(dat$Y))
meanvec = pl$int + as.vector(pl$beta %*% t(dat$X))
sdObs = exp(pl$logSdObs)

for(i in 1:length(dat$Y)){
  log_lik[i] = dnorm(dat$Y[i], meanvec[i], sdObs, log=TRUE)
}

lppd = sum(log(mean(exp(log_lik))))
p_waic = sum(var(log_lik))
waic = -2 * (lppd - p_waic)
print(waic)

# loo
loo_data = matrix(log_lik, ncol = 1)

loo_result = loo(loo_data)
print(loo_result)
```

# Day 3

## Espatial data

```{r}
#load data
datafile = "data/Thy example.csv"
rdata = read.table(file=datafile, header=TRUE, sep = ",")
rdata$Grazing = as.integer(rdata$Grazing)    
rdata$Wetness = as.integer(rdata$Wetness)
rdata$Hits = as.integer(rdata$Hits1)    
summary(rdata)

# prepare pallete
pal = colorBin("viridis", bins = c(0,1,5,10,15,20,25))
leaflet(rdata) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircles(lng = ~East, lat = ~North, color = ~pal(Hits)) %>%
  addLegend("bottomright",
            pal = pal, value = ~Hits,
            title = "Spatial Distribution of values"
  ) %>%
  addScaleBar(position = c("bottomleft"))

#coordinate matrix
coords = as.matrix(rdata[,2:3])


#mesh
bnd = fm_nonconvex_hull(coords,convex=-0.1)
mesh = fm_mesh_2d(boundary=bnd)
#previous wrong version
# mesh<-fm_mesh_2d(boundary=bnd,max.edge=c(0.5,1.5))

plot(mesh,main="")
points(coords,pch=21,bg=1,col="white",cex=1.8)

spde = fm_fem(mesh)
projObs = fm_basis (mesh , loc = coords)

#Betabinomial distribution - code from Anders

dbetabinom. = function (x, size, prob, rho = 0, log = FALSE) 
{
  dbetabinom.ab.(y = x, n = size, a = prob * (1 - rho)/rho, 
                 b = (1 - prob) * (1 - rho)/rho, log = log)
}

dbetabinom.ab. = function(y, a, b, n, log=FALSE){
  
  #  Wikipedia:
  #  f(k|n,\alpha,\beta) =
  #  \frac{\Gamma(n+1)}{\Gamma(k+1)\Gamma(n-k+1)}
  #  \frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(n+\alpha+\beta)}
  #  \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
  
  logres = lgamma(n + 1) - lgamma(y + 1)     - lgamma(n - y + 1) +
    lgamma(y + a) + lgamma(n - y + b) - lgamma(n + a + b) +
    lgamma(a + b) - lgamma(a) - lgamma(b) 
  if(!log){exp(logres)}else{logres}
}

dbetabinom.(10, 25, 0.1, rho = 0.1, log = TRUE)

#dat and par list

dat<-list()
dat$hits <- rdata$Hits
dat$grazing <- as.integer(rdata$Grazing)
dat$wetness <- as.integer(rdata$Wetness)
dat$mesh <- mesh
dat$spde <- spde
dat$projObs <- projObs

par<-list()
par$beta <- as.vector(rep(0, 3))
par$logitRho <- 0
par$logTau <- 0
par$logKappa <- 0
par$omega <- as.vector(rep(0, dat$mesh$n))

jnll <- function(par){
  getAll(par, dat)
  rho <- exp(logitRho)/(1+exp(logitRho))
  tau <- exp(logTau)
  kappa <- exp(logKappa)
  jnll <- 0
  Q <- tau*(kappa^4*spde$c0+2*kappa^2*spde$g1+spde$g2)
  jnll <- jnll -dgmrf(omega, Q=Q, log = TRUE)
  pomega <- projObs %*% omega
  for(i in 1:length(hits)){
    predlin <- beta[1]+beta[2]*grazing[i]+beta[3]*wetness[i]+pomega[i]
    predp <- exp(predlin)/(1+exp(predlin))
    jnll <- jnll - dbetabinom.(hits[i], 25, predp, rho = rho, log = TRUE) 

  }
  jnll
}

jnll(par)

obj = MakeADFun(jnll, par, random="omega")

fit = nlminb(obj$par, obj$fn, obj$gr, control=list(eval.max=10000, iter.max=10000))

sdr <- sdreport(obj)
pl <- as.list(sdr,"Est")
plsd <- as.list(sdr,"Std")

pl$omega

pdf("data/Thy map.pdf")
omegaS1<-exp(pl$omega+pl$beta[1])
cc<-viridis::mako(500)
tc<-apply(col2rgb(cc)/255, 2, function(x)rgb(x[1],x[2],x[3],.6))
plot(mesh, col="gray", lwd=.1)
nt<-nrow(mesh$graph$tv)
lamv<-apply(mesh$graph$tv,1,function(idx)mean(omegaS1[idx]))
lamc<-tc[as.numeric(cut(lamv,breaks = 500))]
dummy<-sapply(1:nrow(mesh$graph$tv),function(i){idx<-mesh$graph$tv[i,];polygon(mesh$loc[idx,1], mesh$loc[idx,2], col=lamc[i], border=NA)})
fields::image.plot(as.matrix(omegaS1), col=tc, type="n", legend.only=TRUE, horizontal=TRUE)
title(sub("Log","","Hits"))

dev.off()
```

## Example of waste water

```{r}
Data = read_xlsx("data/Substances.xlsx")
Substance = "Bly"
DataSub = subset(Data,ParameterName==Substance)
dat = list()
dat$logC = log(DataSub$ValueNew) # response variable
dat$D = DataSub$Attribute
dat$stat = as.integer(as.factor(DataSub$Name))
dat$kombi = as.integer(as.factor(DataSub$KombiTypeTilAfloeb))
dat$InOut = as.integer(as.factor(DataSub$MeasurementPointType))
par = list()
par$logSdObs = 0
#par$logSdS <- 0
par$logSdk = 0
#par$Co<-0
par$Zs = numeric(max(dat$InOut))
par$Zk = numeric(max(dat$kombi))

jnll = function(par){
  getAll(par, dat)
  sdObs = exp(logSdObs)
  #sdS <- exp(logSdS)
  sdk <- exp(logSdk)
  prediction <- numeric(length(logC))
  jnll <- 0
  jnll <- jnll -sum(dnorm(Zk, 0,sdk, log=TRUE))  
  for(i in 1:length(logC)){
    if(!is.na(logC[i])){
      predLog <- Zs[InOut[i]]+Zk[kombi[i]]
      prediction[i] <- predLog 
      if(dat$D[i]=="="){  
        jnll <- jnll - dnorm(logC[i], predLog, sdObs, log=TRUE)
      }
      if(dat$D[i]=="<"){
        jnll <- jnll - log(pnorm(logC[i],predLog,sdObs))
      }
    }
  }
  REPORT(prediction)
  jnll
}



obj <- MakeADFun(jnll, par, random=c("Zs","Zk"))
fit <- nlminb(obj$par, obj$fn, obj$gr)

sdr <- sdreport(obj)
pl <- as.list(sdr,"Est")
plsd <- as.list(sdr,"Std")

#idx <- which(dat$ParameterName==Substance)
idx<-1:length(dat$logC)
p <- obj$report()$prediction[idx]
o <- dat$logC[idx]
plot(p,o)
model<-lm(o~p)
abline(model)
hist(pl$Zk)
```

# Day 4

No scripts were worked on today. Only lectures and personal work